{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b03459a0-53c5-49fb-be71-d2c1dcab6d33",
   "metadata": {},
   "source": [
    "# Проект 1799: \"Разработка системы предсказания параметров цифровых схем с использованием методов машинного обучения\"  \n",
    "## Первые шаги, в предсказании Площади и Задержки схем"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a53b90-ecf9-46e7-9921-665405b3172c",
   "metadata": {},
   "source": [
    "## Подготовка и исследование данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53703121-95d3-4381-bade-453e836f4c62",
   "metadata": {},
   "source": [
    "### Загрузка и очистка от выбросов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "94ed9d73-92ea-4457-867d-b5920adc300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "259313d1-9a45-4b80-882e-cc8fa421fdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка данных\n",
    "data = pd.read_csv('data_optimized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "682fb99e-7473-4d10-9c03-4df7f92dc0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13824 entries, 0 to 13823\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   File       13824 non-null  object \n",
      " 1   Area       13520 non-null  float64\n",
      " 2   Delay      13520 non-null  float64\n",
      " 3   embedding  13520 non-null  object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 432.1+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c765c2d-ec95-49ca-87c1-2bdbe1314406",
   "metadata": {},
   "source": [
    "Видим, что имеются пропуски в значениях т.к. количество ненулевых значений в стобцах Area, Delay и embadding не совпадает с общим числом значений. Значения, которые имеют пропуски в этих ячейках, для обучения не подходят. Очистим данные от пропусков. Важно не забыть, что после использования функции(метода) dropna нумерация будет не последовательной, что может привести к ошибкам в будущем. Стоит сразу же это исправить"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1f99feeb-0b20-4887-b272-2b7ca6cbaa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dropna(inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0ef91ed5-9bdf-4c78-b259-6f97fb4b66fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13520 entries, 0 to 13519\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   File       13520 non-null  object \n",
      " 1   Area       13520 non-null  float64\n",
      " 2   Delay      13520 non-null  float64\n",
      " 3   embedding  13520 non-null  object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 422.6+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e3a14-db58-4ce2-b8cb-a15a54120649",
   "metadata": {},
   "source": [
    "Проверим наличие выбросов в значениях параметров Задержки и площади:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "df502141-2b7e-4c3c-bcde-07ac4fb41173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальная задержка:  1078.17\n",
      "Минимальная задержка:  -1000000000.0\n",
      "Максимальная площадь:  1189.38\n",
      "Минимальная площадь:  0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Максимальная задержка: \", data['Delay'].max())\n",
    "print(\"Минимальная задержка: \", data['Delay'].min())\n",
    "print(\"Максимальная площадь: \", data['Area'].max())\n",
    "print(\"Минимальная площадь: \", data['Area'].min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d961a8-cb40-4f2b-8905-90dc913877b1",
   "metadata": {},
   "source": [
    "Видим, что в таблице присутствуют некорректные данные, с площадью равной 0 и устремленным в минус бесконечность временем задержки. Для улучшения качества обучения, необходимо избавиться от таких данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9560941e-862e-460b-ae78-5a5282f63794",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Delay'] == data['Delay'].min()].index, inplace=True)\n",
    "data.drop(data[data['Area'] == data['Area'].min()].index, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "154960b1-3e62-45b2-b73c-1be2066fb60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13092 entries, 0 to 13091\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   File       13092 non-null  object \n",
      " 1   Area       13092 non-null  float64\n",
      " 2   Delay      13092 non-null  float64\n",
      " 3   embedding  13092 non-null  object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 409.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696af25c-58d3-474f-979c-9e7591d56c0b",
   "metadata": {},
   "source": [
    "### Проверка корректности данных. Поиск выбросов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8f86d-57f6-4c7d-bf1c-5c10b664bbad",
   "metadata": {},
   "source": [
    "В этом месте, необходимо произвести оценку данных. Вывести графическую статистику по Area и Delay. Задача - определить значения, которые являются выбросами и удалить эти строки из DF. Таким образом, данные будут лучше подобраны для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6e4581fd-32d0-4480-b368-ab404d9a39de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # подключаем библиотеку\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ac0b5c07-fc73-43fd-b822-75dcff72f4af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2) = plt.subplots( #делаем полотно с несколькими графиками\n",
    "    nrows=1, ncols=2, # расположенными в один ряд два столбца\n",
    "    figsize=(8, 4) # размер полотна\n",
    ") #\n",
    "\n",
    "ax1.hist( # строим второй график - гистограмму\n",
    "    data['Delay'], bins=15, # задаем данные и диапазоны\n",
    "    label=('x', 'y') # и подписи данных\n",
    ")\n",
    "\n",
    "ax1.legend(loc=(0.65, 0.8)) # отображаем легенду и задаем ее расположение\n",
    "ax1.set_title('Spreading the delay values') # название второго графика\n",
    "ax1.yaxis.tick_right() # устанавливаем, что вертикальная ось должна быть справа\n",
    "\n",
    "ax2.hist( # строим второй график - гистограмму\n",
    "    data['Area'], bins=20,  # задаем данные и диапазоны\n",
    "    label=('x', 'y') # и подписи данных\n",
    ")\n",
    "\n",
    "ax2.locator_params (axis='x', nbins=10)\n",
    "ax2.legend(loc=(0.65, 0.8)) # отображаем легенду и задаем ее расположение\n",
    "ax2.set_title('Spreading the area values') # название второго графика\n",
    "ax2.yaxis.tick_right() # устанавливаем, что вертикальная ось должна быть справа\n",
    "\n",
    "plt.show() # отображаем полотна"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d7a269-f8a2-4f36-a0b4-5d45a2a3e748",
   "metadata": {},
   "source": [
    "Видим, что значения задержи, большие 1000 (Если данных будет нехватать, то и большие 750), можно принять за выбросы. Для площади, это значения большие 150."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "22de5e0b-3545-4930-84b8-902eca3cc488",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data[data['Delay'] > 1000].index, inplace=True)\n",
    "data.drop(data[data['Area'] > 600].index, inplace=True)\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "df6af5a9-72e0-4a94-9689-ab06049856b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12845 entries, 0 to 12844\n",
      "Data columns (total 4 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   File       12845 non-null  object \n",
      " 1   Area       12845 non-null  float64\n",
      " 2   Delay      12845 non-null  float64\n",
      " 3   embedding  12845 non-null  object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 401.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a82ab7-ccba-4a1f-91d3-5338ae609fe2",
   "metadata": {},
   "source": [
    "### Парсинг эмбеддингов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2753a106-eb6d-4325-8079-236562d02ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[[-0.0023294328711926937, 0.0010059750638902187, 0.020557958632707596, 0.036383241415023804, -0.0376126691699028, -0.028824374079704285, 0.025990357622504234, 0.03626422584056854, -0.02031301148235798, -0.01526272390037775, 0.029781151562929153, -0.006153405643999577, -0.01836802251636982, 0.026345035061240196, -0.01960602030158043, -0.007308585569262505, 0.011734742671251297, 0.004220206756144762, -0.03347805514931679, -0.03825681284070015, 0.02961454726755619, 0.020517736673355103, 0.027350598946213722, 0.0030764532275497913, 0.025808831676840782], [-0.013696731068193913, -0.003834116505458951, 0.023253051564097404, -0.030173856765031815, -0.01584583893418312, -0.030236484482884407, -0.0036846150178462267, 0.03843872249126434, -0.02941228821873665, -0.009418928995728493, -0.007723821792751551, 0.032445743680000305, -0.023836053907871246, 0.00016100586799439043, -0.019183043390512466, -0.038594264537096024, 0.020150167867541313, -0.03511103242635727, -0.017706187441945076, -0.00011774320591939613, -0.0011518558021634817, -0.030753539875149727, 0.03862890228629112, 0.01997707411646843, 0.03714706748723984], [-0.03294512629508972, 0.018175216391682625, -0.01659122295677662, 0.00327205378562212, 0.034152813255786896, -0.018111038953065872, 0.018167775124311447, -0.027258407324552536, -0.01430983655154705, 0.03776699677109718, -0.006367519032210112, 0.0013824488269165158, -0.016851194202899933, -0.03094335086643696, -0.00604605209082365, 0.009969803504645824, -0.003496497869491577, 0.0222238190472126, -0.011048825457692146, 0.009159495122730732, 0.022046102210879326, 0.03354886174201965, -0.005744418129324913, -0.03702332079410553, 0.017710795626044273], [0.0021920744329690933, 0.030043257400393486, -0.0031905469950288534, -0.010634770616889, -0.03550097346305847, -0.003643881529569626, 0.011354581452906132, 0.021913593634963036, 0.028372613713145256, -0.02311408892273903, 0.007508694659918547, 0.02467852458357811, -0.01956838183104992, -0.012615450657904148, 0.027345456182956696, 0.006661360152065754, 0.0009288702858611941, 0.014027981087565422, 0.0008323406218551099, 0.038807518780231476, 0.020526479929685593, -0.035910844802856445, -0.028234215453267097, 0.0037786311004310846, 0.025954050943255424], [-0.03480219841003418, 0.01476800162345171, 0.020837295800447464, 0.023035233840346336, 0.02988561801612377, -0.024934792891144753, 0.0044600265100598335, 0.02443135902285576, -0.01147161703556776, -0.024879351258277893, -0.0016629527090117335, -0.0335087850689888, -0.022588321939110756, 0.028482630848884583, 0.01345396414399147, 0.02892482653260231, 0.027384474873542786, 0.030318085104227066, -0.015286985784769058, -0.002374842297285795, 0.009549268521368504, -0.01817435212433338, 0.03383982181549072, -0.039564136415719986, 0.02742743119597435], [0.011657667346298695, -0.019731326028704643, 0.01759275048971176, -0.006958298850804567, 0.0268455371260643, 0.03985939919948578, -0.017449773848056793, -0.002397351199761033, -0.022782549262046814, 0.01540328934788704, 0.01114650722593069, 0.027564305812120438, 0.024404382333159447, 0.038153987377882004, 0.037093669176101685, 0.031592272222042084, -0.027958016842603683, -0.03662345930933952, -0.0014230108354240656, -0.012399363331496716, 0.03157726675271988, 0.023754296824336052, -0.006182651501148939, 0.0060438537038862705, 0.007160163018852472], [0.031221413984894753, -0.03800676017999649, -0.0008251278777606785, 0.013892829418182373, -0.003753352677449584, 0.03350015729665756, 0.03603631258010864, 0.02614651434123516, -0.0029082733672112226, 0.030851740390062332, -0.03413693234324455, 0.012855597771704197, -0.018581824377179146, -0.020369209349155426, 0.014370223507285118, 0.021484730765223503, 0.031122101470828056, -0.023043490946292877, 0.029682885855436325, 0.026488279923796654, -0.014825999736785889, -0.03497131168842316, 0.02178771421313286, 0.026014655828475952, -0.00309141562320292], [-0.026838170364499092, -0.028321318328380585, -0.010008350014686584, 0.0205688439309597, -0.01466402132064104, -0.037491243332624435, 0.01529657281935215, 0.01955600641667843, -0.025740202516317368, 0.004857406951487064, -0.008305797353386879, 0.00010143716644961387, -0.0395485945045948, 0.010766460560262203, -0.0189965907484293, 0.0043498375453054905, -0.0062868958339095116, 0.008785965852439404, -0.03153669089078903, -0.01087137870490551, 0.010672013275325298, 0.021398205310106277, -0.009581784717738628, -0.03803852200508118, 0.018033983185887337], [0.0003692518803291023, 0.012304848060011864, -0.027250973507761955, -0.0054757301695644855, 0.03068890981376171, 0.02940233238041401, -0.014683234505355358, 0.010551521554589272, -0.03326330706477165, 0.024816997349262238, -0.01854083128273487, -0.012665962800383568, 0.037268225103616714, 0.0034873199183493853, 0.029972588643431664, -0.024279607459902763, 0.020644506439566612, 0.039738137274980545, -0.033839087933301926, -0.02054847590625286, -0.028264906257390976, -0.01943659409880638, -0.01512074377387762, -0.03417346999049187, 0.0318157859146595], [-0.0193757526576519, 0.03369445353746414, 0.021050281822681427, -0.026200104504823685, 0.015831485390663147, 0.02188059873878956, -0.029706144705414772, -0.0296228788793087, -0.009900922887027264, -0.03450290113687515, -0.006326289381831884, -0.0016137313796207309, 0.013198737986385822, 0.0057675219140946865, -0.003525686217471957, -0.022376231849193573, 0.006921463180333376, -0.0035894871689379215, 0.02717476338148117, 0.015894360840320587, 0.018117886036634445, 0.0057372236624360085, -0.010799421928822994, -0.017467251047492027, -0.004128298722207546], [0.005748109892010689, -0.010584034956991673, -0.028295131400227547, -0.03122122772037983, -0.0364871472120285, -0.023740677163004875, -0.007389698177576065, -0.017295485362410545, -0.025842681527137756, -0.014869289472699165, 0.017156634479761124, -0.014956173487007618, 0.033512700349092484, 0.006135974079370499, -0.028969278559088707, 0.037735193967819214, 0.030524849891662598, 0.021973127499222755, -0.027395382523536682, 0.023290716111660004, 0.016036372631788254, 0.020741477608680725, 0.017023606225848198, 0.0077590178698301315, -0.012680649757385254], [0.03340898081660271, 0.03845127299427986, 0.015166639350354671, -0.011342647485435009, 2.4800037863315083e-05, 0.0048750462010502815, -0.033833954483270645, -0.03289102017879486, -0.000934199255425483, 0.004944471176713705, -0.02297208085656166, -0.018895957618951797, -0.02938186191022396, 0.03330874815583229, 0.000489143596496433, -0.018036045134067535, 0.022818559780716896, 0.03673696517944336, -0.016411224380135536, 0.031851183623075485, 0.021502627059817314, 0.02351311966776848, 0.0020583528093993664, 0.0328495129942894, -0.028065457940101624], [-0.033003322780132294, 0.03720344230532646, -0.0007693423540331423, -0.007846744731068611, 0.018428966403007507, -0.016388123854994774, 0.01097151916474104, 0.027746396139264107, 0.024231746792793274, -0.03002784214913845, 0.037530336529016495, 0.018703248351812363, 0.01583612710237503, -0.02499259263277054, 0.03384258970618248, -0.008597718551754951, 0.0353161096572876, -0.02144409902393818, -0.0325327031314373, 0.027325648814439774, 0.006674242205917835, -0.008791071362793446, 0.03806423023343086, 0.03793346881866455, -0.039052799344062805], [0.010029474273324013, 0.02463032677769661, 0.01548205316066742, 0.008089660666882992, 0.0017177468398585916, 0.0026999893598258495, -0.015278203412890434, -0.028558772057294846, -0.00836135633289814, 0.01570054329931736, 0.03527550771832466, 0.03703812137246132, -0.02389773540198803, -0.03760653734207153, 0.03906074911355972, 0.013714980334043503, 0.020665571093559265, 0.025130202993750572, -0.01122744008898735, 0.029284430667757988, 0.011320791207253933, 0.011481008492410183, -0.009524906985461712, -0.0125110549852252, -0.009481065906584263], [0.01726079173386097, 0.00018494292453397065, -0.038535986095666885, -0.038846150040626526, -0.024864915758371353, -0.0005071103223599494, 0.008189044892787933, 0.03828215226531029, 0.02252020873129368, -0.017414892092347145, 0.00122460862621665, 0.019995225593447685, 0.031164418905973434, -0.004561795387417078, 0.017388952895998955, -0.023514162749052048, -0.003195103956386447, 0.032901033759117126, -0.009686673060059547, -0.039058052003383636, 0.02328014001250267, -0.015857748687267303, -0.004884709604084492, 0.04015952721238136, -0.008955453522503376], [-0.01904181018471718, -0.02130647376179695, 0.02792889066040516, -0.02283521182835102, 0.008463786914944649, -0.021029559895396233, 0.024477140977978706, 0.017437487840652466, 0.010418218560516834, -0.005967398174107075, -0.010983305983245373, 0.03597339242696762, 0.020853865891695023, -0.00865125097334385, -0.037867721170186996, -0.029710300266742706, -0.004245086573064327, -0.0031700688414275646, -0.010255379602313042, 0.03873365372419357, -0.001831100438721478, 0.02348080649971962, -0.02978574112057686, -0.0100405877456069, -0.022179489955306053], [-0.028556060045957565, 0.004964122548699379, -0.02870686538517475, -0.008978472091257572, 0.014877214096486568, 0.023332495242357254, 0.004792733117938042, 0.008410925976932049, -0.01644156500697136, 0.02890133298933506, -0.025228166952729225, 0.01858886331319809, -0.032879892736673355, 0.008145871572196484, -0.019908208400011063, -0.016990752890706062, -0.01243593730032444, 0.022620834410190582, 0.02319360338151455, -0.01989859528839588, 0.0030933236703276634, -0.03398311138153076, 0.03123922273516655, 0.037029165774583817, -0.010969310067594051], [0.0032008932903409004, 0.0029866076074540615, 0.021911539137363434, -0.0344243161380291, 0.002337822923436761, 0.0274776890873909, 0.00892637763172388, 0.004498705733567476, -0.03728862106800079, 0.03392946720123291, -0.0250565093010664, -0.011969494633376598, 0.01397514808923006, -0.0030905103776603937, 0.005645165219902992, 0.007127966731786728, -0.027315597981214523, -0.038899246603250504, 0.03616233915090561, 0.024792218580842018, -0.0276517104357481, 0.013613929972052574, 0.0008242559270001948, 0.019014982506632805, -0.028479771688580513], [0.016105299815535545, 0.01739473268389702, 0.039824843406677246, -0.017893580719828606, -0.00556453550234437, -0.029274804517626762, -0.038788460195064545, -0.036305367946624756, -0.004097627010196447, -0.02601435035467148, 0.01940060965716839, -0.024645835161209106, 0.010072590783238411, 0.0029552956111729145, -0.01356158684939146, -0.003920457791537046, 0.03992493450641632, 0.036588724702596664, -0.017856158316135406, 0.036327917128801346, -0.02255941927433014, 0.023719677701592445, -0.012384644709527493, 0.013729632832109928, 0.012078380212187767], [0.027589984238147736, -0.009490713477134705, 0.03508953005075455, 0.030355997383594513, -0.03820107877254486, -0.032042164355516434, -0.030551403760910034, 0.011712954379618168, -0.011194069869816303, -0.027725733816623688, -0.03250616043806076, 0.033251721411943436, 0.007963264361023903, -0.03731647506356239, -0.019168367609381676, 0.012540262192487717, -0.018838705494999886, 0.021140918135643005, -0.016953611746430397, 0.010546117089688778, -0.032170675694942474, 0.024837229400873184, 0.019292065873742104, 0.0031551083084195852, 0.012070880271494389], [-0.034915123134851456, 0.008526120334863663, -0.003495933022350073, -0.03727622330188751, -0.03771492838859558, -0.005646794103085995, 0.017729628831148148, 0.014821087941527367, -0.025998925790190697, -0.02749442122876644, -0.019996412098407745, -0.009141736663877964, -0.029004819691181183, -0.03841685876250267, -0.010971972718834877, -0.0334518700838089, -0.02415044605731964, -0.022676916792988777, -0.009382165037095547, -0.006826941855251789, -0.03582341596484184, -0.002943851053714752, 0.03261271119117737, 0.0307600200176239, -0.028815878555178642], [-0.014687805436551571, 0.012497957795858383, -0.0382952056825161, 0.005885886028409004, 0.026092922315001488, 0.022963697090744972, -0.03504041209816933, -0.018045250326395035, -0.032573167234659195, 0.00018721030210144818, 0.037051718682050705, 0.02392154186964035, 0.020257780328392982, 0.020222898572683334, -0.012958417646586895, 0.03819785639643669, -0.029405910521745682, -0.029048334807157516, -0.00909267459064722, -0.003112781560048461, -0.012840542942285538, -0.0023762157652527094, 0.029957160353660583, -0.0027964410837739706, -0.006462818011641502], [0.010975377634167671, -0.033436693251132965, 0.03142407536506653, 0.034143585711717606, -0.03833671286702156, 0.009784537367522717, 0.03962155804038048, -0.030659567564725876, -0.027868803590536118, -0.03094800002872944, 0.033583689481019974, -0.0027210344560444355, 0.03657934442162514, -0.03263641893863678, 0.014973539859056473, 0.010538577102124691, 0.0029739837627857924, 0.009316260926425457, -0.02988130785524845, -0.03743219003081322, 0.009417104534804821, 0.02459181286394596, 0.031945426017045975, 0.02294286899268627, -0.003105316311120987], [0.03322465717792511, -0.03734525665640831, 0.013624530285596848, 0.0010670137125998735, 0.015428977087140083, 0.029543133452534676, -0.02690066769719124, 0.02233792282640934, -0.03808889910578728, -0.003217835444957018, -0.03475494682788849, -0.020394692197442055, 0.037156905978918076, -0.007433047518134117, 0.011657705530524254, 0.03628511726856232, 0.03575253114104271, -0.03283374011516571, -0.012049254961311817, 0.0395464226603508, 0.020417723804712296, -0.006352348253130913, -0.034768085926771164, 0.011846065521240234, -0.026703590527176857], [0.032529085874557495, -0.01782933622598648, -0.004273429047316313, 0.004025459289550781, -0.0007644558208994567, 0.004592709708958864, 0.024455443024635315, -8.108616020763293e-05, -0.01298386137932539, -0.006042914465069771, 0.02358919568359852, 0.006056408863514662, -0.0028970479033887386, 0.037332989275455475, -0.019685134291648865, -0.0033536385744810104, 0.03670164570212364, 0.026997709646821022, 0.006011424120515585, -0.03553024306893349, 0.004594983998686075, -0.009153022430837154, 0.03747294843196869, 0.004839711356908083, 0.005960254464298487], [0.009625639766454697, -0.007344026584178209, -0.01999853551387787, 0.0009297180222347379, -0.008056721650063992, 0.02640373259782791, 0.03576049208641052, -0.0026990175247192383, 0.011908059008419514, -0.024430617690086365, 0.00679729925468564, -0.027704929932951927, -0.03477610647678375, -0.023600811138749123, -0.03582590073347092, 0.0291103795170784, -0.0230881255120039, 0.033105406910181046, -0.028974181041121483, 0.01368669979274273, 0.03869999572634697, -0.031141791492700577, -0.039780229330062866, -0.017316585406661034, -0.01073252223432064], [-0.0010851573897525668, -0.03532620519399643, -0.03447023034095764, 0.011200842447578907, -0.03282562643289566, -0.036277346312999725, -0.009361863136291504, -0.03452723100781441, -0.028226599097251892, -0.03360460326075554, -0.0012053155805915594, -0.01825719326734543, 0.026508698239922523, 0.006108641624450684, -0.013365902937948704, 0.02443588711321354, -0.024053139612078667, -0.018624678254127502, -0.028830036520957947, -0.017346320673823357, -0.0072373198345303535, 0.025958571583032608, -0.011081571690738201, 0.01967586949467659, 0.027617769315838814], [-0.029854822903871536, 0.018259400501847267, 0.02450791373848915, -0.011817898601293564, 0.02650008723139763, 0.024503517895936966, -0.025773940607905388, -0.027058206498622894, 0.010155835188925266, -0.006495275534689426, -0.024260511621832848, 0.03799683600664139, -0.020520586520433426, -0.02621638774871826, -0.0004795408167410642, -0.010805712081491947, 0.001777601195499301, -0.014149832539260387, -0.0016773224342614412, -0.0028344630263745785, 0.003291282569989562, 0.0327792689204216, -0.022946828976273537, -0.006638112012296915, 0.02228643000125885], [0.03267247974872589, -0.0177721306681633, 0.03594173491001129, 0.03301465883851051, -0.01774088852107525, 0.0012124204076826572, 0.017097964882850647, -0.015705280005931854, -0.022239862009882927, -0.026049289852380753, -0.0026829529087990522, -0.0011836863122880459, 0.017852339893579483, -0.009896215982735157, -0.0006904363399371505, 0.009847503155469894, 0.019470395520329475, -0.00012323379633016884, -0.02535763755440712, -0.03704322874546051, 0.00010663032298907638, 0.026647577062249184, 0.0058640907518565655, -0.035866089165210724, -0.03175441920757294], [0.026207609102129936, -0.015142722055315971, 0.025019969791173935, -0.026724128052592278, 0.03391864895820618, -0.026065297424793243, 0.013152079656720161, -0.004227943252772093, -0.0271501112729311, -0.01315038651227951, -0.004645647946745157, -0.021883759647607803, -0.004845390096306801, -0.030253253877162933, 0.010586638003587723, 0.036280594766139984, -0.009509000927209854, -0.003906040219590068, 0.014054246246814728, 0.034660350531339645, -0.023687411099672318, -0.027550311759114265, -0.011731939390301704, 0.036590784788131714, 0.0034650706220418215], [-0.03471360355615616, -0.005787916015833616, 0.037917863577604294, -0.030197950080037117, -0.02143239416182041, 0.03726625069975853, -0.03589490428566933, 0.015303630381822586, 0.0026617622934281826, 0.026642804965376854, 0.03325101360678673, -0.011403140611946583, -0.015969252213835716, 0.035591669380664825, 0.00835858378559351, 0.024995766580104828, -0.037782859057188034, 0.03836049512028694, -0.005393233150243759, -0.02420846000313759, 0.011970138177275658, -0.0018264437094330788, 0.018825970590114594, -0.00913208443671465, -0.016551369801163673], [0.009111595340073109, 0.033417534083127975, -0.01998242363333702, 0.010674715042114258, -0.03196221962571144, -0.02709338627755642, -0.0018706751288846135, -0.035070911049842834, 0.011157751083374023, 0.006394381634891033, -0.009278769604861736, 0.020015163347125053, 0.03899514675140381, 0.033817071467638016, -0.0075208996422588825, 0.008232607506215572, -0.01601475663483143, -0.03296562284231186, 0.02511182241141796, -0.007796726189553738, -0.002664818661287427, -0.0070853279903531075, -0.018142662942409515, 0.016246838495135307, -0.017080722376704216], [-0.03831418603658676, 0.03577246144413948, 0.01666027493774891, 0.03693893924355507, 0.026574010029435158, 0.011698947288095951, 0.03921607881784439, -0.017698563635349274, -0.027213243767619133, 0.0169095229357481, 0.014915999956429005, -0.022658443078398705, 0.03881904110312462, -0.014233226887881756, 0.03819762542843819, 0.0033389043528586626, -0.025353826582431793, -0.007908468134701252, -0.02950821816921234, -0.011918092146515846, 0.004166788887232542, 0.037930749356746674, 0.03742339089512825, -0.02638350985944271, 0.013900604099035263], [0.009102282114326954, -0.009957408532500267, -0.036916688084602356, 0.0041085053235292435, -0.03266282379627228, 0.025280756875872612, -0.023200321942567825, 0.022141756489872932, 0.039334893226623535, -0.0006400013226084411, 0.01811397075653076, -0.007237601093947887, 0.02944304421544075, 0.015760388225317, -0.036041297018527985, -0.009594015777111053, 0.014515075832605362, -0.00039827346336096525, -0.0048050833866000175, -0.004221754148602486, -0.006686406210064888, 0.002419810276478529, 0.016660381108522415, -0.017011165618896484, -0.015334486961364746], [-0.00021126747014932334, 0.0010774231050163507, -0.0006752252811565995, -0.01914202608168125, 0.017253609374165535, -0.008687677793204784, 0.008414158597588539, 0.00266609201207757, 0.023878708481788635, -0.02736952342092991, -0.027262840420007706, -0.01790503039956093, 0.03774331510066986, -0.006367530673742294, -0.03771696984767914, -0.002180166309699416, -0.017795691266655922, 0.024000314995646477, -0.038334742188453674, 0.01143600419163704, -0.03701132908463478, 0.004999203607439995, 0.02399679273366928, 0.029589390382170677, -0.0304858535528183], [-0.024212094023823738, -0.027353763580322266, -0.031673360615968704, -0.0379963219165802, -0.008501987904310226, -0.0033437300007790327, -0.02902480587363243, 0.02714814618229866, 0.004478478338569403, 0.023315466940402985, 0.005891466047614813, 0.0031574631575495005, -0.02947251871228218, -0.008706632070243359, 0.017284316942095757, -0.020341258496046066, 0.004523158073425293, 0.011533455923199654, -0.006145443767309189, 0.03972918167710304, 0.03339853882789612, 0.009662666358053684, 0.028472982347011566, 0.02356575056910515, -0.02232246845960617], [-0.020624971017241478, -0.026673350483179092, -0.0311073400080204, 0.03324291110038757, -0.007929367944598198, -0.02741985395550728, -0.016617555171251297, 0.02057650499045849, -0.011476545594632626, -0.01499864086508751, 0.006485700607299805, -0.011105175130069256, -0.006337442435324192, 0.004297967068850994, -0.011911778710782528, 0.034077130258083344, 0.015643740072846413, -0.039835453033447266, 0.025038398802280426, -0.0270170159637928, 0.0030777312349528074, 0.017616938799619675, -0.02041349932551384, -0.008442678488790989, 0.03238191455602646], [-0.016975170001387596, -0.03054504469037056, 0.03703165054321289, -0.008621987886726856, -0.018877729773521423, 0.03428319841623306, 0.017133355140686035, 0.0172993466258049, 0.03713805228471756, -0.03381232172250748, 0.0210212804377079, 0.008157406002283096, 0.016753120347857475, 0.0067916009575128555, 0.0178565364331007, 0.01794516108930111, 0.024418087676167488, -0.012808389961719513, -0.018302911892533302, -0.0017060660757124424, 0.01013493537902832, -0.013052668422460556, 0.024230871349573135, 0.016616521403193474, 0.03105836920440197], [0.010277089662849903, 0.03246673196554184, -0.005548839457333088, 0.032311711460351944, 0.014868063852190971, -0.032189298421144485, -0.015734458342194557, -0.00988751370459795, 0.019572176039218903, -0.0034886361099779606, -0.01132363360375166, 0.031334828585386276, 0.03729160130023956, -0.006459712982177734, -0.020636998116970062, -0.01880703493952751, -0.019384203478693962, -0.038411304354667664, 0.005488080903887749, -0.01689966209232807, 0.010106835514307022, 0.02245793305337429, -0.016263622790575027, -0.038386326283216476, 0.00618679029867053], [-0.02680048905313015, 0.009980697184801102, -0.015122528187930584, 0.028313685208559036, 0.0025608825962990522, 0.014243755489587784, -0.01095653511583805, -0.006842189002782106, 0.030611172318458557, 0.005630712490528822, -0.023401794955134392, -0.03133798763155937, 0.004930749069899321, 0.02581852488219738, 0.02222539857029915, -0.03590819984674454, 0.03436865657567978, 0.016187911853194237, 0.029878420755267143, 0.03898533806204796, -0.029158320277929306, -0.03615985065698624, 0.023344002664089203, 0.037564851343631744, 0.014027700759470463], [0.0283551886677742, -0.006271719932556152, 0.03178999572992325, -0.03795463591814041, -0.032117996364831924, -0.02656148374080658, -0.016013817861676216, 0.019956864416599274, -0.01525423489511013, -0.03327962011098862, 0.03364710882306099, -0.014988007955253124, 0.034434784203767776, -0.01958300545811653, 0.015674376860260963, 0.019688067957758904, 0.009570436552166939, -0.011275215074419975, 0.011396498419344425, -0.03302494436502457, -0.011062159202992916, -0.010364633053541183, 0.028996024280786514, -0.01385361235588789, -0.026398811489343643], [0.017361707985401154, -0.001897940644994378, -0.014390225522220135, 0.027529887855052948, 0.015489249490201473, -0.015600805170834064, 0.003087553894147277, 0.03657400980591774, 0.03101862408220768, 0.025447487831115723, 0.018669210374355316, 0.009537959471344948, -0.007366504520177841, -0.025485172867774963, -0.0012072420213371515, -0.00626155361533165, -0.00228914269246161, -0.025051483884453773, 0.029736189171671867, -0.026365971192717552, -0.02895710989832878, -0.011028585024178028, -0.00606160145252943, -0.030542869120836258, 0.0027929639909416437], [-0.02130444534122944, -0.005102176684886217, -0.029460445046424866, 0.007842273451387882, 0.013092794455587864, -9.255409531760961e-05, -0.021793432533740997, -0.006904344540089369, 0.02833966724574566, 0.014945034869015217, -0.03552419692277908, -0.013654203154146671, 0.009416408836841583, 0.00855207908898592, -0.03785603120923042, 0.018284663558006287, -0.03462798893451691, -0.029548272490501404, 0.013932447880506516, -0.013883833773434162, 0.0142578836530447, 0.035576362162828445, -0.01429728977382183, 0.03728169947862625, 0.006844153627753258], [0.03939109668135643, 0.02282017283141613, -0.03659793362021446, -0.013310923241078854, 0.02612069994211197, 0.02241111733019352, 0.03482206165790558, 0.027704410254955292, 0.03215555101633072, -0.03929203376173973, 0.01719530113041401, -0.020120305940508842, 0.014049543999135494, 0.024226751178503036, 0.017568526789546013, 0.030049437656998634, 0.005990862846374512, -0.005059766583144665, 0.023073602467775345, -0.022558270022273064, 0.00015436649846378714, 0.03782634809613228, -0.021925000473856926, 0.015257115475833416, -0.03245208412408829], [0.03908117115497589, 0.032660454511642456, 0.005123887211084366, 0.020390314981341362, 0.00563251506537199, -0.025820646435022354, -0.005712204147130251, 0.025796661153435707, -0.018469223752617836, -0.015972262248396873, 0.01969761773943901, 0.010852393694221973, -0.007391901221126318, -0.011507773771882057, 0.024042926728725433, -0.022866955026984215, -0.012946810573339462, -0.02595129981637001, -0.016938529908657074, -0.034323979169130325, -0.01787915639579296, -0.03404491767287254, 0.005615110509097576, -0.034472785890102386, -0.03966662287712097], [-0.03306926414370537, -0.027245959267020226, 0.026952847838401794, 0.015280717052519321, 0.0013345214538276196, -0.011977155692875385, -0.029857350513339043, 0.002285380382090807, 0.001950949663296342, 0.0006813124637119472, 0.0034994278103113174, 0.00319509650580585, -0.00031038507586345077, -0.03225735202431679, -0.023699773475527763, -0.03375774994492531, -0.0051955655217170715, 0.0074445707723498344, 0.029751108959317207, -0.007922819815576077, -0.009296179749071598, 0.03812149539589882, 0.0003821880090981722, -0.009708295576274395, 0.034776464104652405], [0.010712087154388428, -0.02144186571240425, 0.02646716497838497, 0.018108580261468887, -0.028400031849741936, -0.0013684382429346442, 0.0033877529203891754, 0.023192934691905975, -0.006930242292582989, -0.011339239776134491, 0.007045446429401636, 0.0034792828373610973, 0.0047734701074659824, -0.010639685206115246, -0.024032846093177795, 0.029395556077361107, 0.030559461563825607, 0.033464595675468445, -0.03464512899518013, 0.010551083832979202, -0.014240497723221779, 0.038630276918411255, 0.011733314022421837, 0.01864684373140335, 0.009714302606880665], [0.02643391117453575, -0.022973161190748215, 0.03157765045762062, -0.00964368786662817, -0.018247542902827263, -0.00824396125972271, 0.03893423080444336, -0.02742636203765869, -0.008766880258917809, 0.0280039981007576, -0.00022299766715150326, -0.025179868564009666, -0.025574102997779846, 0.03576157987117767, 0.02571830339729786, 0.019094372168183327, -0.013048190623521805, -0.03707047924399376, 0.015147552825510502, 0.02864220179617405, -0.022531557828187943, -0.031460050493478775, -0.011890959925949574, -0.019727593287825584, -0.009260444901883602], [-0.0077768657356500626, -0.02107008546590805, 0.037788454443216324, -0.037194930016994476, 0.018015790730714798, 0.02161671221256256, -0.005637049674987793, 0.03602837026119232, 0.03954143822193146, -0.021900171414017677, -0.024083999916911125, -0.026987891644239426, -0.031579528003931046, -0.012191667221486568, -0.022376108914613724, -0.0333787202835083, 0.003131608944386244, 0.011978626251220703, 0.025658974424004555, -0.01051579974591732, -0.01781390607357025, 0.004998283460736275, 0.001565847429446876, 0.03246799483895302, 0.0007312011439353228], [0.028926344588398933, -0.03305806219577789, 0.033734146505594254, -0.0075556375086307526, 0.03480461612343788, -0.03046734817326069, 0.007185544818639755, 0.004225945565849543, 0.0001840210024965927, -0.02041301317512989, -0.03699079155921936, -0.029056869447231293, -0.03180469572544098, 0.00765490997582674, 0.0019138669595122337, -0.007252550218254328, 0.028480663895606995, -0.009902767837047577, -0.0053892373107373714, -0.035602256655693054, -0.03970165178179741, 0.035797592252492905, -0.02301575243473053, -0.02549199014902115, 0.02079762890934944], [0.026679974049329758, -0.027326565235853195, 0.003839039709419012, -0.024033894762396812, 0.006589374504983425, -0.017157115042209625, -0.013763189315795898, 0.008742665871977806, 0.03464630991220474, 0.026912441477179527, -0.03870822861790657, -0.022488417103886604, 0.031521331518888474, 0.007957429625093937, -0.0170242078602314, 0.002395248506218195, 0.038083843886852264, -0.00441086757928133, -0.03769855201244354, 0.0064336396753787994, 0.02492941915988922, 0.02512948028743267, 0.01636660099029541, -0.02260095626115799, -0.0014827728737145662], [-0.0002212715189671144, 0.018287181854248047, -0.03216635808348656, -0.032073237001895905, 0.0010590028250589967, -0.03443319723010063, 0.02328062616288662, -0.0016712475335225463, 0.03988470882177353, -0.021375909447669983, -0.0019445562502369285, 0.031027093529701233, -0.01627172902226448, -0.020063601434230804, 0.006360283121466637, 0.010602775029838085, -0.010259837843477726, 0.02579011395573616, -0.030639810487627983, 0.01357424259185791, 0.0019598817452788353, 0.03492873162031174, 0.023930855095386505, 0.02726144716143608, 0.03129017725586891], [-0.038113873451948166, 0.03829694166779518, -0.031065763905644417, -0.01050554122775793, -0.019725089892745018, -0.01994207501411438, -0.03209913522005081, -0.03107961267232895, -0.01823972351849079, -0.005163113120943308, -0.02033468708395958, 0.024632249027490616, -0.03812185674905777, -0.021287165582180023, 0.03779805451631546, 0.027917101979255676, 0.030756087973713875, 0.017066726461052895, 0.0019786355551332235, -0.024022547528147697, 0.024167165160179138, 0.010588487610220909, 0.03086312673985958, 0.025559216737747192, 0.031950876116752625], [0.03462962806224823, -0.03958301991224289, -0.027022819966077805, 0.005350298713892698, 0.025761213153600693, 0.02949526347219944, 0.022067923098802567, 0.03064650110900402, -0.020502271130681038, 0.0263376384973526, -0.016433492302894592, -0.03622137010097504, 0.03656671941280365, 0.0053256177343428135, -0.011038717813789845, -0.0099113704636693, -0.0168819148093462, 0.01924934796988964, 0.01760088838636875, -0.010613450780510902, -0.02936752326786518, -0.014263405464589596, -0.0013464593794196844, 0.02438356913626194, -0.011349353939294815], [-0.0004835796426050365, 0.003518905723467469, -0.028382597491145134, 0.00826000701636076, -0.00572969438508153, 0.011208596639335155, 0.019368886947631836, -0.005408077035099268, -0.011120571754872799, 0.030954590067267418, 0.020182395353913307, 0.02685408666729927, 0.018062572926282883, 0.03466862067580223, 0.029899878427386284, -0.0043275500647723675, 0.03499055281281471, 0.01840687356889248, 0.021762533113360405, -0.005544338375329971, -0.008165273815393448, -0.01769738271832466, -0.03406078740954399, 0.012150912545621395, 0.035532768815755844], [0.03567896783351898, -0.007769417949020863, 0.024344630539417267, 0.015118894167244434, -0.017183862626552582, 0.008171696215867996, -0.021751541644334793, 0.032835569232702255, 0.021731633692979813, 0.012737737037241459, 0.01641027443110943, 0.034628599882125854, 0.029088139533996582, -0.0033338929060846567, -0.028291072696447372, 0.03352189064025879, 0.028934335336089134, 0.006921887397766113, -0.005389967001974583, -0.02356036752462387, -0.018132362514734268, 0.03459186479449272, -0.012540430761873722, -0.025355277583003044, 0.039480309933423996], [0.030786585062742233, 0.036482568830251694, 0.004542007576674223, -0.03330031782388687, 0.033700063824653625, -0.014784922823309898, 0.02296869270503521, 0.017566317692399025, 0.038759779185056686, -0.03717399016022682, 0.03683362156152725, -0.03712611272931099, -0.027630848810076714, -0.03640877828001976, -0.022188439965248108, 0.029475584626197815, 0.0366579107940197, -0.013301406055688858, 0.014892201870679855, -0.0145008135586977, 0.031525883823633194, 0.023467503488063812, 8.344650268554688e-07, -0.014514698646962643, -0.028897223994135857], [0.019074464216828346, 0.0058119152672588825, -0.01045274268835783, 0.031351227313280106, -0.016198458150029182, -0.036595944315195084, -0.009021882899105549, 0.0005005884449928999, -0.026557020843029022, -0.021946463733911514, -0.03399910777807236, 0.0369194932281971, 0.029696112498641014, -0.0011809730203822255, 0.0294706542044878, 0.031803153455257416, -0.003134293481707573, 0.026448363438248634, 0.015070094726979733, 0.020307369530200958, 0.029011964797973633, -0.018957557156682014, -0.008742132224142551, 0.003492493648082018, 0.016944823786616325], [0.013217325322329998, 0.020383309572935104, 0.018345942720770836, -0.03375403583049774, -0.012735357508063316, -0.02894703857600689, 0.03872568905353546, 0.020026396960020065, 0.0006833648658357561, 0.016451912000775337, -0.030624523758888245, -0.02517860382795334, 0.012305574491620064, 0.026138553395867348, 0.015799498185515404, 0.024072088301181793, -0.007944527082145214, -0.013380518183112144, 0.0008286809897981584, -0.012777443043887615, -0.022067617624998093, -0.031154241412878036, 0.026142172515392303, -0.0043613482266664505, -0.007563519291579723], [-0.031219100579619408, 0.03735029324889183, 0.003472566604614258, 0.0070785474963486195, 0.009966664016246796, -0.02954397164285183, 0.006555290427058935, 0.01190625224262476, -0.03426811844110489, 0.019823208451271057, 0.009733634069561958, 0.029991650953888893, 0.020177192986011505, -0.012126865796744823, -0.028651747852563858, 0.028384853154420853, 0.0076061394065618515, 0.020796943455934525, 0.025524435564875603, 0.007649116683751345, -0.024510445073246956, -2.518653855076991e-05, 0.03307319059967995, -0.024394191801548004, 0.03775312379002571], [-0.028763771057128906, 0.0169315617531538, 0.008653578348457813, 0.029762858524918556, -0.019557060673832893, -0.0182573851197958, -0.02439269609749317, 0.013197469525039196, -0.017997851595282555, 0.034091539680957794, -0.01715530827641487, -0.036421678960323334, -0.019265422597527504, 0.02566596120595932, -0.025485295802354813, -0.021046146750450134, -0.029217643663287163, 0.024089045822620392, 0.01343037560582161, 0.011393561027944088, -0.012554202228784561, 0.02412356436252594, -0.024610981345176697, -0.00792040303349495, -0.023932328447699547], [-0.00398272043094039, -0.008083944208920002, 0.0339437834918499, 0.0003120040928479284, -0.03430130332708359, -0.021716393530368805, -0.02750394307076931, 0.010769524611532688, 0.03782659024000168, -0.02326398342847824, 0.03306010365486145, 0.034128207713365555, -0.0282505564391613, -0.03553285077214241, 0.037876736372709274, 0.03349745646119118, -0.01876356638967991, -0.026904163882136345, 0.031368546187877655, 0.015053382143378258, 0.03238201513886452, -0.030286183580756187, -0.038100339472293854, 0.006309623830020428, -0.039223071187734604], [-0.019543537870049477, -0.013840412721037865, 0.03848369047045708, 0.03449427708983421, -0.011342430487275124, 0.023307491093873978, 0.03294837847352028, -0.009051923640072346, 0.038114167749881744, 0.028640860691666603, 0.008166003040969372, -0.015395054593682289, -0.020326999947428703, -0.012206611223518848, 0.031551457941532135, -0.024761738255620003, -0.01166008971631527, 0.03676408901810646, 0.013826622627675533, 0.024290448054671288, -0.032131265848875046, -0.0030060196295380592, 0.022089790552854538, -0.01885787397623062, 0.029913973063230515], [0.03727836534380913, -0.001637473120354116, -0.008254404179751873, -0.002373433206230402, -0.023143557831645012, -0.033545125275850296, -0.006027493625879288, -0.010228734463453293, 0.01752973534166813, -0.027470069006085396, 0.021654782816767693, -0.026980342343449593, -0.031281158328056335, 0.03388684615492821, 0.035674456506967545, -0.013924960978329182, 0.013965658843517303, -0.023182939738035202, -0.03500034287571907, -0.022062286734580994, 0.026994915679097176, 0.025671038776636124, 0.037752293050289154, 0.028221139684319496, 0.027019809931516647], [0.005200667306780815, -0.03921721130609512, 0.018351050093770027, -0.00215289113111794, 0.0253283828496933, 0.007133898790925741, -0.01251919288188219, 0.0310398917645216, 0.0062186624854803085, 0.00022083759540691972, -0.018451815471053123, -0.033814094960689545, -0.03106732852756977, 0.03468203917145729, -0.035699840635061264, 0.0361388623714447, -0.03712407127022743, -0.0011070251930505037, -0.007628188002854586, -0.03572458401322365, 0.034520238637924194, 0.027111254632472992, 0.012077756226062775, 0.019333811476826668, 0.00044876098399981856], [0.03769872337579727, 0.028085149824619293, -0.0394149050116539, -0.017732882872223854, -0.0051604462787508965, 0.01219089049845934, -0.01729580946266651, 0.005796666257083416, -0.03138359636068344, 0.011112294159829617, 0.01881076768040657, 0.019749250262975693, -0.01270280871540308, -0.03370816260576248, -0.03688247129321098, -0.002891597803682089, -0.02930985949933529, -0.02725985087454319, 0.024480022490024567, 0.02868921309709549, 0.008469676598906517, -0.03159760311245918, -0.02279595285654068, 0.03220738098025322, 0.015683375298976898], [-0.020961880683898926, -0.029567617923021317, 0.0030862188432365656, 0.01385501865297556, 0.008316773921251297, 0.0124032162129879, -0.022482000291347504, -0.03955794498324394, -0.028083348646759987, 0.0009212350705638528, 0.018474716693162918, 0.01810523122549057, 0.007519249804317951, 0.020682698115706444, -0.0004214429936837405, 0.016456665471196175, -0.0364929623901844, 0.03080364689230919, 0.024589896202087402, 0.02049662545323372, 0.028826676309108734, 0.033759187906980515, 0.002954783383756876, -0.006815442815423012, 0.002074513351544738], [-0.03740830346941948, 0.033669427037239075, -0.025539889931678772, 0.033876173198223114, -0.017198093235492706, 0.0025977755431085825, -0.03675072640180588, -0.03821016848087311, -0.031444597989320755, -0.031168628484010696, 0.0016045743832364678, -0.02893030270934105, -0.019795933738350868, -0.02120841294527054, -0.017251254990696907, 0.0280301533639431, 0.019427189603447914, 0.03499053046107292, 0.028393812477588654, -0.022952161729335785, 0.0290666613727808, -0.03735405579209328, -0.010331236757338047, -0.031096717342734337, 0.01696084998548031], [0.007200922816991806, 0.02818436548113823, 0.011778793297708035, -0.027923397719860077, 0.030850710347294807, -0.023955708369612694, 0.03599083423614502, 0.011836819350719452, -0.016061171889305115, -0.018755974248051643, -0.017666887491941452, -0.024585828185081482, 0.03751496225595474, -0.010598382912576199, 0.03108975850045681, -0.038721341639757156, 0.008435163646936417, -0.004934458527714014, 0.030176915228366852, -0.03621840476989746, 0.02975025214254856, -0.02042318321764469, -0.024055099114775658, -0.022596625611186028, -0.013516678474843502], [-0.013644385151565075, -0.012782659381628036, -0.029968805611133575, 0.0028351068031042814, -0.002304286928847432, -0.006735997274518013, 0.015028528869152069, -0.030480757355690002, -0.012885689735412598, 0.020621366798877716, 0.03417545184493065, -0.03923976048827171, 0.02878134697675705, 0.021237950772047043, -0.015518789179623127, 0.034304630011320114, -0.03688796982169151, 0.028994722291827202, 0.02145533636212349, 0.005174379330128431, -0.02079901657998562, -0.016714610159397125, -0.013427119702100754, 0.00643314840272069, 0.0063468171283602715], [0.029552973806858063, 0.03991037234663963, 0.03546934947371483, -0.01602579653263092, 0.03858156129717827, -0.0025181628298014402, 0.019461732357740402, 0.010196065530180931, -0.0025192403700202703, 0.014669789932668209, -0.0212776567786932, -0.023026710376143456, -0.030418576672673225, 0.007625737227499485, 0.02610347792506218, 0.0035285139456391335, 0.005027794744819403, 0.01268638577312231, 0.03253866359591484, -0.030800241976976395, 0.009043007157742977, -0.029896439984440804, 0.014839248731732368, 0.03804219886660576, 0.03008103370666504], [0.025704121217131615, 0.03205910325050354, 0.026204580441117287, 0.02742670476436615, 0.03472835198044777, -0.019792161881923676, 0.03685181587934494, 0.020236806944012642, -0.008520998992025852, 0.03394979611039162, 0.020325370132923126, 0.03859581798315048, 0.011329603381454945, 0.03947015106678009, 0.004788017366081476, 0.03651673346757889, 0.014347868040204048, 0.026259221136569977, -0.014445323497056961, 0.027171636000275612, 0.028974294662475586, -0.008533844724297523, -0.007438216358423233, 0.014446983113884926, -0.02814570814371109], [0.03894219920039177, -0.03912153095006943, -0.025997959077358246, 0.011135144159197807, 0.025727950036525726, -0.021469468250870705, 0.011009969748556614, 0.03648524358868599, -0.027261685580015182, -0.0243996474891901, -0.01995856687426567, -0.014705648645758629, 0.007398862857371569, 0.03873053193092346, 0.025751104578375816, 0.001588368439115584, 0.009883075021207333, 0.03376196324825287, 0.036515917629003525, 0.022515010088682175, 0.023785052821040154, -0.030482744798064232, -0.015310687944293022, -0.02272132784128189, 0.024727096781134605], [-0.009025798179209232, -0.0351177416741848, 0.03047649934887886, 0.03359873220324516, -0.013280940242111683, 0.03646663576364517, -0.002953438786789775, -0.014506077393889427, -0.0015387630555778742, 0.0007777070859447122, -0.014019584283232689, 0.011252965778112411, 0.02291882038116455, 0.02747603878378868, -0.03561386466026306, -0.008770913816988468, -0.021927189081907272, 0.0300843995064497, 0.02600681222975254, -0.017442889511585236, 0.009307332336902618, -0.023814644664525986, 0.0009459829307161272, 0.037847038358449936, -0.010439367033541203], [-0.020750870928168297, -0.029588837176561356, -0.011647768318653107, -0.0034572554286569357, 0.014111447148025036, 0.0389675572514534, -0.013557100668549538, 0.007607073988765478, 0.038724035024642944, 0.006126346532255411, 0.003945984877645969, 0.03920948505401611, 0.03718183934688568, 0.030832266435027122, -0.02468210645020008, 0.039935942739248276, 0.023395948112010956, 0.03629066422581673, -0.00798079464584589, 0.013399744406342506, 0.02733422815799713, -0.015575027093291283, 0.026571402326226234, 0.010251450352370739, 0.03725491836667061], [-0.012143216095864773, -0.01243747677654028, 0.02486155927181244, -0.03631298243999481, -0.02901596575975418, -0.02600010484457016, -0.0029962968546897173, -0.009452085942029953, 0.02726207673549652, 0.0369463674724102, -0.0036390351597219706, 0.005651297513395548, 0.008081426844000816, -0.008079195395112038, -0.03213736414909363, 0.029764199629426003, -0.017191605642437935, 0.018306074663996696, 0.03635881468653679, 0.012172889895737171, 0.01255516055971384, 0.01624731533229351, -0.010804867371916771, 0.015299077145755291, 0.0013504934031516314], [0.022506847977638245, 0.021989483386278152, 0.007316479459404945, 0.022997627034783363, -0.035872310400009155, 0.026237430050969124, 0.03690396621823311, -0.016828589141368866, 0.006430201698094606, -0.020935526117682457, 0.0042328741401433945, 0.011080674827098846, 0.032642945647239685, 0.002176051028072834, 0.01022823341190815, 0.005190940108150244, 0.03361009061336517, -0.022830810397863388, -0.025047320872545242, -0.014510073699057102, -0.009202199056744576, 0.02016425132751465, -0.03248142823576927, -0.01133414264768362, -0.03278970718383789], [0.020598839968442917, -0.010272255167365074, -0.03626842796802521, 0.01628691703081131, 0.036069292575120926, -0.012150640599429607, -0.02335415780544281, 0.012079553678631783, -0.0017433929024264216, -0.03991774469614029, 0.0336708165705204, -0.029355550184845924, -0.019721627235412598, -0.010628324002027512, -0.021809257566928864, 0.006866040173918009, 0.03885125741362572, 0.01828908920288086, 0.032354410737752914, -0.001881833071820438, 0.0025796936824917793, -0.010673408396542072, -0.03511824458837509, 0.013725213706493378, 0.008373494260013103], [-0.03768741711974144, -0.01987374760210514, -0.038936395198106766, -0.022879166528582573, 0.01625816896557808, 0.03457144275307655, 0.01644659973680973, 0.009553857147693634, 0.03257911279797554, -0.004476838279515505, -0.005590853746980429, -0.03498729318380356, -0.0005031680921092629, -0.010270290076732635, 0.001544308615848422, 0.029118647798895836, -0.028165841475129128, -0.01578589901328087, -0.026658421382308006, -0.014176459051668644, -0.01326332613825798, 0.008548484183847904, 0.013312673196196556, -0.01982874795794487, -0.018185162916779518], [0.004554776940494776, 0.021813930943608284, 0.02149459905922413, -0.01187414675951004, -0.017066102474927902, -0.022466588765382767, -0.0021799325477331877, 0.007785492110997438, 0.0061013842932879925, 0.029410118237137794, -0.01093348953872919, -0.00026369572151452303, -0.022110532969236374, -0.004680261481553316, -0.030847854912281036, -0.003837318392470479, 0.005238699726760387, -0.03437897562980652, 0.03499433398246765, -0.03683146461844444, -0.03849870711565018, -0.03404649719595909, 0.029253073036670685, 0.021862274035811424, 0.03699784725904465], [0.010280709713697433, 0.0033861161209642887, -0.010157761164009571, 0.03743067383766174, 0.011030035093426704, 0.016382593661546707, -0.004731740802526474, 0.003618016140535474, 0.02649722993373871, -0.0029060030356049538, 0.013366031460464, -0.002688083564862609, 0.020993590354919434, 0.014559736475348473, 0.010330119170248508, -0.021239571273326874, -0.018832402303814888, 0.01723208837211132, -0.023628683760762215, -0.0007208252209238708, -0.0025340651627629995, 0.013972973451018333, -0.03376906365156174, 0.035268936306238174, -0.005794978234916925], [-0.021323708817362785, 0.016217876225709915, -0.007741961628198624, -0.0310619305819273, -0.017987193539738655, -0.0015509080840274692, -0.035785891115665436, 0.0022822523023933172, 0.00976717472076416, -0.012901091948151588, 0.01028160098940134, 0.00992319080978632, 0.03995286300778389, 0.005722022149711847, 0.008082151412963867, 0.01111824531108141, -0.00831148587167263, -0.03479446470737457, 0.03209269419312477, -0.007898788899183273, -0.038769256323575974, -0.026208018884062767, -0.015783444046974182, 0.015817388892173767, 0.020154323428869247], [0.02434404380619526, -0.027081040665507317, 0.0027553511317819357, -0.011101126670837402, -0.020836787298321724, 0.027925090864300728, 0.0158078670501709, -0.012426028028130531, -0.03310762718319893, -0.020563144236803055, -0.002598137827590108, 0.031247057020664215, 0.024176741018891335, -0.03380662575364113, -0.03825414180755615, 0.02853534184396267, -0.009310264140367508, -0.0147692346945405, 0.02299932949244976, -0.023377222940325737, 0.020375819876790047, -0.0009697008063085377, -0.027500653639435768, -0.001326551428064704, 0.02543596737086773], [0.03717575594782829, 0.008883652277290821, 0.020200371742248535, -0.01989809423685074, -0.003190207527950406, -0.021270541474223137, 0.004755849950015545, -0.007172856479883194, -0.014530539512634277, -0.028057517483830452, 0.03861772641539574, 0.01190331019461155, -0.00912489928305149, -0.016736360266804695, 0.030855150893330574, -0.02592741884291172, 0.012486171908676624, 0.003149428404867649, 0.0332915224134922, 0.027344709262251854, -0.011634588241577148, 0.010140400379896164, -0.0066642905585467815, -0.03781864792108536, -0.0104538444429636], [0.005330095067620277, 0.02616342529654503, 0.039938412606716156, 0.03624982014298439, -0.032061442732810974, 0.02596554346382618, -0.02285894937813282, -0.0038862992078065872, 0.0019312906078994274, 0.02632773481309414, 0.017880573868751526, 0.018414907157421112, 0.03793249651789665, 0.001530656823888421, -0.02415485307574272, -0.025320401415228844, 0.025727158412337303, -0.02097032591700554, -0.011399250477552414, 0.01630113646388054, -0.009160943329334259, -0.024100851267576218, -0.009294619783759117, 0.004827675875276327, 0.00873343925923109], [0.024335093796253204, -0.02085600420832634, 0.01231184508651495, 0.02896270342171192, 0.008780455216765404, 0.021589960902929306, -0.019381333142518997, 0.024610448628664017, -0.03040512651205063, 0.013971461914479733, -0.03728721663355827, -0.010417241603136063, -0.03629279509186745, -0.006353073287755251, -0.021459007635712624, -0.01577567495405674, 0.004614610690623522, 0.011201390996575356, -0.0061055803671479225, -0.032682061195373535, -0.023672088980674744, 0.0032771825790405273, -0.015785031020641327, -0.03772179037332535, -0.003099899273365736], [0.02653307467699051, 0.023915376514196396, -0.039669036865234375, 0.012474332004785538, -0.02394920401275158, -0.03672756254673004, 0.0006808519246987998, -0.001478481339290738, -0.027887998148798943, -0.02512023039162159, -0.009704342111945152, 0.02838662639260292, -0.030183516442775726, 0.03079233691096306, -0.0019017792074009776, 0.0043731736950576305, 0.03793064504861832, 0.01891263946890831, -0.01429106667637825, 0.014942479319870472, 0.014077143743634224, 0.025346789509058, 0.0002773570886347443, -0.01769474521279335, 0.005273084621876478], [-0.021656937897205353, 0.0056470297276973724, 0.019704723730683327, 0.02061939239501953, 0.03672654926776886, -0.03005065955221653, -0.021614575758576393, 0.025877490639686584, 0.005421915091574192, -0.026447204872965813, 0.0035375594161450863, 0.010708160698413849, -0.01011534221470356, -0.01984064094722271, 0.02001902088522911, 0.03848728537559509, -0.02945869415998459, -0.0004612875054590404, -0.010257835499942303, -0.025460567325353622, -0.005514621734619141, -0.02100314199924469, 0.03623998165130615, -0.023163074627518654, 0.01474430039525032], [-0.0009499407024122775, 0.01687670685350895, 0.008456583134829998, 0.03998328745365143, 0.0025179244112223387, -0.02185770496726036, -0.004718771204352379, 0.008282127790153027, -0.013492274098098278, -0.03138706833124161, -0.02239704690873623, -0.027000270783901215, 0.025413308292627335, 0.01569022610783577, 0.032862141728401184, 0.02607867307960987, -0.024511361494660378, 0.010863805189728737, 0.03388850390911102, 0.006382627412676811, 0.012272915802896023, 0.023250455036759377, -0.035357486456632614, 0.036498989909887314, 0.027277931571006775], [0.03404177352786064, -0.03290504962205887, 0.024702157825231552, 0.02651628479361534, -0.005430569872260094, -0.025175461545586586, 0.02130671963095665, -0.027438310906291008, -0.021293416619300842, 0.014046759344637394, 0.032323889434337616, 0.03477077558636665, -0.01762126013636589, -0.03675508126616478, 0.038430776447057724, 0.02516123279929161, -0.01586534082889557, -0.033838916569948196, -0.018873339518904686, -0.01582890935242176, -0.013093804940581322, 0.0032763672061264515, -0.0011886501451954246, -0.012449507601559162, -0.023983949795365334], [0.037682078778743744, -0.01890214905142784, -0.029048394411802292, 0.030738921836018562, 0.010070782154798508, 0.034505583345890045, -0.01786046475172043, -0.027569614350795746, 0.003939609508961439, -0.0046919058077037334, -0.03758825734257698, -0.006412291433662176, 0.012202396057546139, 0.026263127103447914, 0.027380967512726784, 0.01283311378210783, -0.017773089930415154, -0.007356667425483465, -0.015761032700538635, 0.023095659911632538, -0.025434836745262146, 0.008412075228989124, -0.005380935501307249, -0.023251114413142204, -0.02898484654724598], [0.02341024950146675, -0.033432818949222565, -0.002748999511823058, 0.01130970474332571, 0.03095603920519352, -0.029191631823778152, 0.013218292966485023, 0.039233990013599396, -0.027908315882086754, -0.014139432460069656, 0.020534053444862366, 0.02095354162156582, 0.006496457848697901, 0.03188453987240791, 0.003326354082673788, 0.007485265843570232, -0.006420998368412256, -0.03266511112451553, 0.013031215406954288, 0.00786545779556036, -0.0349368192255497, -0.0027000284753739834, 0.0003066825738642365, -2.3837090338929556e-05, 0.03484053537249565], [-0.010035066865384579, -0.02361062914133072, 0.029933348298072815, -0.02903893031179905, -0.03586447611451149, -0.007158374879509211, -0.03346974775195122, 0.0015608310932293534, 0.007782936096191406, -0.009877934120595455, -0.025792039930820465, -0.0012847185134887695, -0.004394016228616238, 0.013973445631563663, 0.03250805288553238, 0.023422284051775932, 0.03376395255327225, -0.035872682929039, 0.037769947201013565, -0.009463024325668812, 0.03478820621967316, 0.009538468904793262, 0.014340963214635849, -0.038326751440763474, -0.037945814430713654], [0.03593568503856659, -0.011587071232497692, 0.01126567367464304, 0.025677772238850594, -0.0011997079709544778, 0.03888758644461632, -0.004134859889745712, -0.03869149833917618, -0.02821939066052437, -0.004172625485807657, -0.03469960391521454, 0.029685793444514275, 0.01447733398526907, -0.034987784922122955, 0.03391889110207558, 0.03571855649352074, 0.023389477282762527, 0.027639413252472878, -0.03828365355730057, 0.0019698715768754482, -0.03688252344727516, -0.014664067886769772, 0.0010126781417056918, -0.001121625886298716, 0.0058611296117305756], [0.013110174797475338, 0.008507709950208664, 0.021279387176036835, 0.030081981793045998, -0.023542756214737892, 0.03182926028966904, 0.023966031149029732, 0.039143551141023636, 0.018495431169867516, -0.01330910250544548, -0.014985628426074982, -0.0025164938997477293, -0.006620793137699366, 0.039490342140197754, 0.004492816980928183, 0.01602901890873909, 0.01367422565817833, -0.03540521115064621, 0.0268824715167284, 0.03276412561535835, -0.006663365289568901, 0.009333815425634384, -0.02591685764491558, -0.02450302056968212, 0.03286417946219444], [-0.012181677855551243, 0.026960128918290138, 0.006212739739567041, -0.007926254533231258, 0.012221312150359154, -0.016084222123026848, -0.0071417237631976604, 0.00552291888743639, 0.019548725336790085, -0.005620894487947226, 0.002626118715852499, -0.0317298024892807, 0.03798062354326248, -0.035050202161073685, 0.028279680758714676, 0.015620551072061062, -0.02799227647483349, -0.021180711686611176, -0.03175298869609833, -0.020521115511655807, 0.028282366693019867, 0.03856247290968895, 0.008618449792265892, 0.0025616216007620096, 0.03809788078069687], [-0.01989426650106907, -0.005133218597620726, 0.013122549280524254, -0.02565613202750683, -0.038806356489658356, -0.03704093396663666, 0.03608278930187225, 0.02148677408695221, -0.01915290765464306, -0.033318568021059036, 0.005175805184990168, 0.011512250639498234, -0.004981136415153742, 0.005083484575152397, -0.017285212874412537, 0.019165463745594025, 0.005900740623474121, 0.03551129996776581, -0.039906054735183716, -0.021078281104564667, -0.03641136735677719, -0.0013916731113567948, -0.031429223716259, 0.02012496441602707, -0.025587420910596848], [-0.023811349645256996, 0.020283641293644905, -0.03263907507061958, 0.005820808466523886, -0.028958164155483246, 0.03944968804717064, 0.034535035490989685, 0.007075810339301825, 0.023154014721512794, 0.018384862691164017, -0.023967131972312927, 0.03902778774499893, -0.03872882202267647, 0.03219703212380409, 0.011025519110262394, -0.012220487929880619, -0.014247450977563858, 0.0362878143787384, -0.0217636339366436, 0.03274748846888542, -0.02403554879128933, 0.03356550633907318, -0.0022219752427190542, 0.03177039697766304, -0.012619886547327042], [0.023916859179735184, 0.03521738201379776, 0.010175351984798908, 0.005270996131002903, 0.020156769081950188, 0.03201008960604668, 0.03427205607295036, 0.033971089869737625, 0.028210105374455452, 0.032010599970817566, 0.03439896181225777, -0.00013236522499937564, -0.00401492603123188, 0.006663165055215359, 1.3093947927700356e-05, 0.002740707481279969, -0.03440374881029129, -0.03837892413139343, -0.009258704259991646, 0.03571280092000961, -0.014590349048376083, -0.027912773191928864, 0.019517531618475914, 0.004276471212506294, 0.00740408431738615], [0.014611826278269291, 0.014082694426178932, 0.02290448732674122, 0.004937200341373682, 0.0033778524957597256, 0.0361810028553009, 0.011128868907690048, -0.01881142146885395, 0.02616874687373638, 0.020853247493505478, 0.011482267640531063, -0.01255133654922247, 0.013347344473004341, 0.025457192212343216, 0.02832416072487831, 0.0037646626587957144, -0.034127067774534225, 0.001031045918352902, 0.0014816808979958296, 0.015771927312016487, -0.037875838577747345, 0.03883148357272148, -0.027889108285307884, 0.02304575964808464, -0.0377194881439209], [0.03859411180019379, 0.029299316927790642, 0.005046625155955553, -0.013620943762362003, -0.00180263991933316, 0.0016904926160350442, -0.025636820122599602, 0.02298063226044178, 0.009485607035458088, 0.015111741609871387, -0.029020071029663086, 0.03410516306757927, 0.002033014316111803, -0.0008157062693499029, -0.036286428570747375, 0.01619502156972885, 0.027053741738200188, 0.02942037023603916, -0.02567010000348091, -0.03142423927783966, -0.022094855085015297, -0.002411589724943042, -0.033351920545101166, -0.03296046331524849, -0.007670278660953045], [0.004557147156447172, -0.03802619129419327, -0.014931735582649708, 0.0025791216176003218, 0.027239860966801643, 0.0069399643689394, -0.0025295305531471968, -0.029925579205155373, -0.026968378573656082, -0.0027777147479355335, 0.029870329424738884, 0.02177147939801216, -0.005930762272328138, 0.004689235705882311, -0.03842310607433319, -0.005537982098758221, -0.01850489154458046, 0.02323724329471588, -0.00935204979032278, -0.01905626244843006, -0.03790045902132988, -0.004801258910447359, -0.02879107929766178, -0.0067408704198896885, -0.01627929136157036], [-0.009496540762484074, -0.012993340380489826, -0.032625891268253326, -0.00499364361166954, 0.006763773038983345, -0.016190577298402786, -0.030565809458494186, -0.01434654276818037, -0.03618853539228439, -0.003033699933439493, 0.023532018065452576, -0.011845383793115616, 0.012647032737731934, 0.019982872530817986, 0.03387371078133583, 0.02248566225171089, 0.03803418576717377, -0.03858672454953194, -0.031850699335336685, -0.027032623067498207, -0.029870834201574326, -0.03185447305440903, -0.031152676790952682, -0.011770505458116531, 0.0055846404284238815], [-0.011739407666027546, -0.03549526259303093, 0.02000526525080204, 0.00355216721072793, 0.01847650669515133, 0.028939753770828247, 0.030824096873402596, -0.0031825685873627663, 0.014711497351527214, -0.020711585879325867, 0.007665765937417746, 0.01839270256459713, 0.03987417370080948, -0.012970373965799809, 0.01150187011808157, -0.023206325247883797, -0.00882356520742178, 0.032959938049316406, -0.015797682106494904, -0.0049455915577709675, -0.03734063729643822, -0.038253143429756165, 0.03591381013393402, -0.022968638688325882, 0.02054625004529953], [-0.027854427695274353, -0.009834046475589275, -0.03209174796938896, 0.030002111569046974, 0.02450966276228428, 0.02103387750685215, 0.03351143002510071, -0.00278613087721169, -0.03725091367959976, 0.03646266460418701, -0.019714144989848137, 0.03139195963740349, 0.02213543839752674, -0.004316306207329035, -0.030656863003969193, -0.005839209537953138, 0.02501414716243744, -0.027864331379532814, 0.005768384784460068, -0.03180743381381035, 0.03488539159297943, -0.011423153802752495, 0.03774920850992203, -0.022832298651337624, -0.03887089714407921], [-0.034511614590883255, -0.01629933901131153, 0.01883837766945362, -0.0009677553316578269, 0.0368940494954586, 0.01243685744702816, 0.014991069212555885, 0.011985396966338158, 0.03259459510445595, -0.009586858563125134, 0.029629355296492577, -0.03814685344696045, 0.01168434601277113, -0.0027266787365078926, 0.001809029607102275, 0.02737203985452652, -0.01136789284646511, -0.009427118115127087, -0.00040190696017816663, -0.0019907664973288774, -0.014299845322966576, 0.02497793175280094, -0.026234669610857964, 0.031567998230457306, -0.00037384033203125], [0.010435361415147781, 0.01289259921759367, -0.0011266135843470693, 0.00682520866394043, -0.012562618590891361, 0.01902582123875618, 0.0009720420930534601, -0.013122382573783398, -0.03485829755663872, -0.0399923175573349, 0.001251111039891839, -0.02298724092543125, -0.004438638687133789, -0.016824373975396156, -0.03455530107021332, 0.004248385317623615, 0.02364436164498329, -0.008843880146741867, -0.028683267533779144, 0.012613749131560326, -0.0015387439634650946, -0.022084569558501244, -0.004422517027705908, -0.0025586176197975874, -0.01273235771805048], [-0.03982030972838402, 0.030554376542568207, 0.014904031530022621, -0.01011685375124216, 0.0292286965996027, 0.0018183612264692783, 0.028692608699202538, -0.006190252490341663, 0.029974671080708504, -0.00017052650218829513, -0.024309445172548294, -0.01886340230703354, 0.03851369768381119, 0.0023242472670972347, 0.004109601955860853, 0.03380114957690239, -0.025153618305921555, -0.0070554353296756744, -0.032715678215026855, -0.026699017733335495, -0.034321948885917664, 0.01572244241833687, 0.0109573844820261, 0.022461652755737305, 0.010287032462656498], [0.008454060181975365, 0.02294062077999115, -0.008465633727610111, 0.012689189985394478, 0.03377504274249077, 0.0015522860921919346, 0.02517833188176155, -0.02697427198290825, 0.026303285732865334, 0.018757905811071396, 0.020321393385529518, -0.039675138890743256, -0.0017177772242575884, 0.012824449688196182, -0.023303957656025887, -0.026775149628520012, 0.025455622002482414, -0.027295002713799477, -0.020729999989271164, 0.032487355172634125, -0.02808118797838688, 0.03418149799108505, 0.002618489321321249, 0.017622828483581543, -0.012049760669469833], [-0.021710939705371857, -0.035790134221315384, -0.002449893858283758, -0.020408950746059418, -0.012304139323532581, 0.032445818185806274, -0.03059164062142372, -0.02523711696267128, -0.021054711192846298, -0.026414962485432625, -0.03536249324679375, -0.0044401646591722965, 0.0377839133143425, 0.0321461483836174, -0.02109638601541519, -0.03255002945661545, -0.009029579348862171, -0.03706712648272514, -0.02027095854282379, -0.023791227489709854, 0.012100487016141415, -0.030276170000433922, 0.005002341233193874, 0.00914760585874319, 0.02125326171517372]]'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['embedding'][1231]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00037e6-01b8-4b87-9048-9ae91fa0393f",
   "metadata": {},
   "source": [
    "Видим, что эмбеддинги для каждой схемы хранятся в формате string, что не подходит для обучения моделей. Необходимо, преобразовать их в то, чем они являются изначально.  \n",
    "Напишем программу, которая приведет текстовое представление данных к list, который будет содержать numpy массивы  \n",
    "Для начала, напишем функцию, которая будет проверять, можно ли приобразовать строку в дробное число (записано ли в строке число)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "1a48bf96-b0b3-4bbf-a8ff-5d1af1381588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_number(s):\n",
    "    try:\n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78976e27-a53c-4663-b074-5acfc5fe5c0f",
   "metadata": {},
   "source": [
    "Далее, напишем функцию, которая распарсит нашу строку на list, который будет содержать numpy массивы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "53450d40-798d-45a5-9fe3-cc4f17db33a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_embedding = []\n",
    "for str in data['embedding']:\n",
    "    result = []\n",
    "    if '\\n' in str:\n",
    "        res = str.split('\\n')\n",
    "    else:\n",
    "        res = str.split('], ')\n",
    "    for mass in res:\n",
    "        s = mass.replace(', dtype=float32',' ').replace('(', ' ').replace(')',' ').replace(',',' ').replace('[','').replace(']','').split(' ')\n",
    "        tmp = []\n",
    "        for word in s:\n",
    "            if is_number(word):\n",
    "                tmp.append(float(word))\n",
    "        result.append(tmp)\n",
    "    r = [np.array(sublist) for sublist in result]\n",
    "    new_embedding.append(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98476c-ea3f-42c5-bec6-21efb3fe6b0f",
   "metadata": {},
   "source": [
    "### Приведение данных в вид, удобный для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd6ad0b-d66a-4436-8a2c-48a982b435d2",
   "metadata": {},
   "source": [
    "Так как каждая схема имеет свое представление в виде графа, возникла проблема, что каждая схема представлена разным количеством векторов длины 4. Т.е. каждая схема описанна массивом, содержащим в себе *n*ое количество массивов длины 4. Так как в процессе обучения, нам требуется, чтобы размерность данных для каждой схемы была единой, то нужно чем-то и до какого-то числа дополнить данные о каждой схеме.  \n",
    "Идеи:  \n",
    "* Находим схему, которая представлена в виде максимального количества векторов. Дополняем все остальные схемы до данной размерности\n",
    "* Дополняем векторами, состоящими из нулей\n",
    "* Дополняем векторами, состоящими из чисел, являющимися средним значением по всей выборке (средними значениями координат векторов)\n",
    "* Дополняем векторами, состоящими из чисел, являющимися средним значением по схемам, имеющим наибольшее число векторов в представлении\n",
    "\n",
    "В данный момент, были реализованы и проверены идеи с дополнением средним значением по всему датасету и дополнение 0. Различия по точности отсутствуют\n",
    "\n",
    "Ниже, представлена реализация дополнения до размерности максимальной из схем, векторами, состоящими из 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9cdacde8-0465-4882-937f-c6f574305190",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_subarrays = max(len(arr) for arr in new_embedding)\n",
    "\n",
    "for i in range(len(new_embedding)):\n",
    "    while len(new_embedding[i]) < max_subarrays:\n",
    "        new_embedding[i] = np.vstack([new_embedding[i], np.zeros(25)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc16b5b-0cde-42a3-a841-84e8f7a4fdd7",
   "metadata": {},
   "source": [
    "Ниже, представленна реализация дополнения до размерности максимальной из схем векторами, состоящими из средних значений по всей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "d387f0b1-4f63-40b0-afe8-3f9c08118d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lst = []\n",
    "#for i in new_embedding:\n",
    "#    for k in i:\n",
    "#        lst.append(k.mean())\n",
    "#val = np.array(lst).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "2495f8da-10bd-4a4c-a2cf-ec10ba9de1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_subarrays = max(len(arr) for arr in new_embedding)\n",
    "#\n",
    "#for i in range(len(new_embedding)):\n",
    "#    while len(new_embedding[i]) < max_subarrays:\n",
    "#        new_embedding[i] = np.vstack([new_embedding[i], np.full(25, val)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4845268-337c-477b-a203-dd22eae1bd34",
   "metadata": {},
   "source": [
    "Так как для большей части алгоритмов машинного обучения данные, представленные в формате двумерного массива не подходят, необходимо, привести их в удобоваримый вид.  \n",
    "Идеи:  \n",
    "* Запишем все имеющиеся в представлении вектора друг за другом, по порядку, по которому они хранятся изначально\n",
    "* Запишем все имеющиеся в представлении вектора друг за другом, перемешав их произвольным образом\n",
    "\n",
    "Ниже, реализован способ, с записыванием вектором друг за другом в исходном порядке   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3d795bbc-6010-42b3-bec1-494e2de9a8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_embedding)):\n",
    "    tmp = []\n",
    "    for j in range (len(new_embedding[i])):\n",
    "        for k in new_embedding[i][j]:\n",
    "            tmp.append(k)\n",
    "    new_embedding[i] = np.array(tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da786dbe-a8e3-4e70-ac27-f999a0f2e726",
   "metadata": {},
   "source": [
    "Добавим в наш датасет новый столбец, содержащий уже преобразованные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "11151307-7398-4faf-9bb4-8c6874a14181",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = pd.Series(new_embedding)\n",
    "data['New_embeddings'] = s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8fb28c-f467-43fd-b194-39bf7110e52c",
   "metadata": {},
   "source": [
    "Теперь, наши данные выглядят следующим образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f310ec03-2fe8-4e19-b5da-33716ada3428",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [-0.0021449089981615543, 0.000945725420024246,...\n",
       "1        [-0.0021449089981615543, 0.000945725420024246,...\n",
       "2        [-0.002141991164535284, 0.0009532237891107798,...\n",
       "3        [-0.0021485977340489626, 0.0009492258541285992...\n",
       "4        [-0.0021652388386428356, 0.0009332413319498301...\n",
       "                               ...                        \n",
       "12840    [-0.0021476016845554113, 0.0009556888835504651...\n",
       "12841    [-0.002182498574256897, 0.0009277667268179357,...\n",
       "12842    [-0.0022188432049006224, 0.0009272933239117265...\n",
       "12843    [-0.0021463329903781414, 0.0009479891159571707...\n",
       "12844    [-0.002146391198039055, 0.0009560753242112696,...\n",
       "Name: New_embeddings, Length: 12845, dtype: object"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['New_embeddings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177c6fca-89aa-4446-aa21-c3f3bb367582",
   "metadata": {},
   "source": [
    "## Подготовка и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8771740-e986-4fe3-a9cf-6aeb0752cfd8",
   "metadata": {},
   "source": [
    "### Крайний этап обработки данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf91cfb-fb1f-478c-946f-70c0098ffd52",
   "metadata": {},
   "source": [
    "Загрузить данные в виде одного столбца нашей исходной таблицы и обучать модели на них - нельзя, из-за требований моделей к формату предсавления обучающей выборки. Поэтому, преобразуем наши данные в таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "0c68b7f6-d48f-4e51-aa09-2d4d3ef907e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>3515</th>\n",
       "      <th>3516</th>\n",
       "      <th>3517</th>\n",
       "      <th>3518</th>\n",
       "      <th>3519</th>\n",
       "      <th>3520</th>\n",
       "      <th>3521</th>\n",
       "      <th>3522</th>\n",
       "      <th>3523</th>\n",
       "      <th>3524</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.002145</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.020413</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>-0.037212</td>\n",
       "      <td>-0.028467</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>-0.020062</td>\n",
       "      <td>-0.015053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.002145</td>\n",
       "      <td>0.000946</td>\n",
       "      <td>0.020413</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>-0.037212</td>\n",
       "      <td>-0.028467</td>\n",
       "      <td>0.025835</td>\n",
       "      <td>0.035892</td>\n",
       "      <td>-0.020062</td>\n",
       "      <td>-0.015053</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.002142</td>\n",
       "      <td>0.000953</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>-0.037219</td>\n",
       "      <td>-0.028467</td>\n",
       "      <td>0.025838</td>\n",
       "      <td>0.035897</td>\n",
       "      <td>-0.020057</td>\n",
       "      <td>-0.015059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.002149</td>\n",
       "      <td>0.000949</td>\n",
       "      <td>0.020408</td>\n",
       "      <td>0.036037</td>\n",
       "      <td>-0.037204</td>\n",
       "      <td>-0.028466</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.035890</td>\n",
       "      <td>-0.020067</td>\n",
       "      <td>-0.015046</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.002165</td>\n",
       "      <td>0.000933</td>\n",
       "      <td>0.020422</td>\n",
       "      <td>0.036041</td>\n",
       "      <td>-0.037203</td>\n",
       "      <td>-0.028482</td>\n",
       "      <td>0.025853</td>\n",
       "      <td>0.035888</td>\n",
       "      <td>-0.020088</td>\n",
       "      <td>-0.015040</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12840</th>\n",
       "      <td>-0.002148</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.020448</td>\n",
       "      <td>0.036086</td>\n",
       "      <td>-0.037270</td>\n",
       "      <td>-0.028437</td>\n",
       "      <td>0.025885</td>\n",
       "      <td>0.035904</td>\n",
       "      <td>-0.020207</td>\n",
       "      <td>-0.015110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12841</th>\n",
       "      <td>-0.002182</td>\n",
       "      <td>0.000928</td>\n",
       "      <td>0.020423</td>\n",
       "      <td>0.036001</td>\n",
       "      <td>-0.037197</td>\n",
       "      <td>-0.028509</td>\n",
       "      <td>0.025925</td>\n",
       "      <td>0.035998</td>\n",
       "      <td>-0.020073</td>\n",
       "      <td>-0.015123</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12842</th>\n",
       "      <td>-0.002219</td>\n",
       "      <td>0.000927</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.036098</td>\n",
       "      <td>-0.037358</td>\n",
       "      <td>-0.028552</td>\n",
       "      <td>0.025927</td>\n",
       "      <td>0.036067</td>\n",
       "      <td>-0.020229</td>\n",
       "      <td>-0.015152</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12843</th>\n",
       "      <td>-0.002146</td>\n",
       "      <td>0.000948</td>\n",
       "      <td>0.020416</td>\n",
       "      <td>0.036038</td>\n",
       "      <td>-0.037214</td>\n",
       "      <td>-0.028468</td>\n",
       "      <td>0.025836</td>\n",
       "      <td>0.035893</td>\n",
       "      <td>-0.020064</td>\n",
       "      <td>-0.015057</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12844</th>\n",
       "      <td>-0.002146</td>\n",
       "      <td>0.000956</td>\n",
       "      <td>0.020441</td>\n",
       "      <td>0.036052</td>\n",
       "      <td>-0.037239</td>\n",
       "      <td>-0.028494</td>\n",
       "      <td>0.025843</td>\n",
       "      <td>0.035914</td>\n",
       "      <td>-0.020088</td>\n",
       "      <td>-0.015059</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12845 rows × 3525 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6     \\\n",
       "0     -0.002145  0.000946  0.020413  0.036037 -0.037212 -0.028467  0.025835   \n",
       "1     -0.002145  0.000946  0.020413  0.036037 -0.037212 -0.028467  0.025835   \n",
       "2     -0.002142  0.000953  0.020416  0.036037 -0.037219 -0.028467  0.025838   \n",
       "3     -0.002149  0.000949  0.020408  0.036037 -0.037204 -0.028466  0.025836   \n",
       "4     -0.002165  0.000933  0.020422  0.036041 -0.037203 -0.028482  0.025853   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "12840 -0.002148  0.000956  0.020448  0.036086 -0.037270 -0.028437  0.025885   \n",
       "12841 -0.002182  0.000928  0.020423  0.036001 -0.037197 -0.028509  0.025925   \n",
       "12842 -0.002219  0.000927  0.020497  0.036098 -0.037358 -0.028552  0.025927   \n",
       "12843 -0.002146  0.000948  0.020416  0.036038 -0.037214 -0.028468  0.025836   \n",
       "12844 -0.002146  0.000956  0.020441  0.036052 -0.037239 -0.028494  0.025843   \n",
       "\n",
       "           7         8         9     ...  3515  3516  3517  3518  3519  3520  \\\n",
       "0      0.035892 -0.020062 -0.015053  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "1      0.035892 -0.020062 -0.015053  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "2      0.035897 -0.020057 -0.015059  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "3      0.035890 -0.020067 -0.015046  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "4      0.035888 -0.020088 -0.015040  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "...         ...       ...       ...  ...   ...   ...   ...   ...   ...   ...   \n",
       "12840  0.035904 -0.020207 -0.015110  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "12841  0.035998 -0.020073 -0.015123  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "12842  0.036067 -0.020229 -0.015152  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "12843  0.035893 -0.020064 -0.015057  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "12844  0.035914 -0.020088 -0.015059  ...   0.0   0.0   0.0   0.0   0.0   0.0   \n",
       "\n",
       "       3521  3522  3523  3524  \n",
       "0       0.0   0.0   0.0   0.0  \n",
       "1       0.0   0.0   0.0   0.0  \n",
       "2       0.0   0.0   0.0   0.0  \n",
       "3       0.0   0.0   0.0   0.0  \n",
       "4       0.0   0.0   0.0   0.0  \n",
       "...     ...   ...   ...   ...  \n",
       "12840   0.0   0.0   0.0   0.0  \n",
       "12841   0.0   0.0   0.0   0.0  \n",
       "12842   0.0   0.0   0.0   0.0  \n",
       "12843   0.0   0.0   0.0   0.0  \n",
       "12844   0.0   0.0   0.0   0.0  \n",
       "\n",
       "[12845 rows x 3525 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = pd.DataFrame(data['New_embeddings'].values.tolist(), index = data['New_embeddings'].index)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b12b533-7917-4e0c-b901-88920f02d24e",
   "metadata": {},
   "source": [
    "## Обучение моделей для предсказания параметра 'Area'\n",
    "Далее, разделим данные на X - матрица объекты-признаки и y - указания учителя  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "03103afd-8d9e-48b5-b2a1-c04693164e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные, для которых будем проводить нормализацию\n",
    "X = tmp\n",
    "# Данные, для которых не будем проводить нормализацию\n",
    "X_0 = tmp\n",
    "#Целевые параметры. Разделим их, для возможных экспериментов с ними в будущем\n",
    "y = data['Area']\n",
    "y_0 = data['Area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8057efdd-d290-445b-958d-2600a00c1e6e",
   "metadata": {},
   "source": [
    "Проведем нормализацию данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "8d43919a-2cd8-4a4c-be03-7cb81111df5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359d0b6c-a5d3-48ca-8a48-2dc867935e59",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочные и тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "98723ec8-3c9a-401b-ba10-d6b75900295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "X_0_train, X_0_test, y_0_train, y_0_test = train_test_split(X_0, y_0, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4044fb4a-8a3c-451d-8d93-a80c60e68542",
   "metadata": {},
   "source": [
    "### Выбор, настройка и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c09fe-9fca-481d-bbaa-4a9778b5da12",
   "metadata": {},
   "source": [
    "### Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "2a102a8c-49d6-4439-917b-b86a0a890fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9827798711731877\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_regressor = GradientBoostingRegressor(learning_rate=0.075, \n",
    "                                         n_estimators=500, \n",
    "                                         max_features='sqrt', \n",
    "                                         random_state=10)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Оценка качества модели на тестовых данных\n",
    "accuracy = gb_regressor.score(X_test, y_test)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6b636f-eb26-4749-a79f-9e529da49a8b",
   "metadata": {},
   "source": [
    "Параметр *accuracy* рассчитывается с помощью метода score объекта GradientBoostingRegressor. В случае регрессии этот метод возвращает коэффициент детерминации (R²), который является мерой качества регрессионной модели.\n",
    "Коэффициент детерминации (R²) рассчитывается по следующей формуле: R^2 = 1 - SS(res)/SS(tot), где\n",
    "SS(res) - сумма квадратов остатков (остаточная сумма квадратов), которая представляет собой сумму квадратов разностей между фактическими значениями и предсказанными значениями.\n",
    "SS(tot) - общая сумма квадратов, которая представляет собой сумму квадратов разностей между фактическими значениями и средним значением фактических значений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "079c747b-3625-4594-ab0f-65c2869b80b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 11.989869453434768\n",
      "Relative Error: 0.09718084339384711\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Предсказание на тестовых данных\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Рассчет средней абсолютной ошибки\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error = mae / abs(y_test.mean())\n",
    "print(\"Relative Error:\", relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4d16cf42-93a5-4614-ba7d-eb1cd8d5b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9817962040034501\n"
     ]
    }
   ],
   "source": [
    "#Обучение без нормализации\n",
    "gb_regressor_0 = GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                         n_estimators=500, \n",
    "                                         max_features='sqrt', \n",
    "                                         random_state=10)\n",
    "gb_regressor_0.fit(X_0_train, y_0_train)\n",
    "\n",
    "# Оценка качества модели на тестовых данных\n",
    "accuracy_0 = gb_regressor_0.score(X_0_test, y_0_test)\n",
    "print(\"Accuracy:\", accuracy_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f39a2df8-5c14-48ef-8739-805a0282bba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error (MAE): 12.423316622396161\n",
      "Relative Error: 0.10069403939735126\n"
     ]
    }
   ],
   "source": [
    "# Предсказание на тестовых данных\n",
    "y_0_pred = gb_regressor_0.predict(X_0_test)\n",
    "\n",
    "# Рассчет средней абсолютной ошибки\n",
    "mae_0 = mean_absolute_error(y_0_test, y_0_pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae_0)\n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error_0 = mae_0 / abs(y_0_test.mean())\n",
    "print(\"Relative Error:\", relative_error_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa9f25c-bfc6-4397-935a-9e217a5c6bdc",
   "metadata": {},
   "source": [
    "### Случайный лес"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ef93b-9e51-4dbc-a84a-e8ab07816d38",
   "metadata": {},
   "source": [
    "На данный момент, присутствуют трудности с подбором параметров (при некоторых значениях модель обучается бесконечно долго) Вполне возможно, что нет необходимости использовать данную модель. Необходимо дальнейшее углубление в принципы работы данной модели и работа с гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "580c537d-dc41-4fdf-aeff-103f75e211b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "#rf_regressor = RandomForestRegressor(n_estimators=200, random_state=0, oob_score=True)\n",
    "\n",
    "#rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "#accuracy = rf_regressor.score(X_test, y_test)\n",
    "#print(\"Accuracy:\", accuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba63582-c1e9-4bee-a76a-cb879932b73c",
   "metadata": {},
   "source": [
    "### Метод опорных векторов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40f100a-9736-4cc4-84f9-dfdeecd736c6",
   "metadata": {},
   "source": [
    "На данный момент, присутствуют трудности с подбором параметров (при некоторых значениях модель обучается бесконечно долго) Вполне возможно, что нет необходимости использовать данную модель. Необходимо дальнейшее углубление в принципы работы данной модели и работа с гиперпараметрами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "bc9bb3d4-9f00-45ac-a172-daefd4531791",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.svm import SVR\n",
    "\n",
    "#svm_regressor = SVR()\n",
    "\n",
    "#param_grid = { 'C':[0.1,1,100,1000],'kernel':['rbf','poly','sigmoid','linear'],'degree':[1,2,3,4,5,6],'gamma': [1, 0.1, 0.01, 0.001, 0.0001]}\n",
    "\n",
    "#grid = GridSearchCV(SVR(),param_grid)\n",
    "\n",
    "#grid.fit(X_train,y_train)\n",
    "\n",
    "#print(grid.best_params_)\n",
    "#print(grid.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5365344-1d31-456a-9d1e-388399e7dd28",
   "metadata": {},
   "source": [
    "### Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "385c3ac1-9a1f-4c0a-8094-dd436c89225d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 28141.0312 - mae: 113.9540 - val_loss: 28119.1602 - val_mae: 110.6877\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 27662.4980 - mae: 112.0494 - val_loss: 28226.7891 - val_mae: 112.4503\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 26133.3438 - mae: 107.5915 - val_loss: 25489.9941 - val_mae: 102.3223\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 23736.8301 - mae: 99.0037 - val_loss: 23940.8223 - val_mae: 97.4568\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 22296.0918 - mae: 93.8055 - val_loss: 22577.0195 - val_mae: 92.1387\n",
      "Epoch 6/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 21051.8184 - mae: 89.5758 - val_loss: 21392.1836 - val_mae: 88.2352\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 19940.6973 - mae: 85.9549 - val_loss: 20329.9043 - val_mae: 84.9433\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 18935.1289 - mae: 82.8553 - val_loss: 19352.1484 - val_mae: 81.8782\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 18018.5938 - mae: 80.0676 - val_loss: 18454.8789 - val_mae: 79.0372\n",
      "Epoch 10/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 17179.2969 - mae: 77.5740 - val_loss: 17638.3047 - val_mae: 76.5736\n",
      "Epoch 11/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 16408.9297 - mae: 75.3312 - val_loss: 16885.2031 - val_mae: 74.4323\n",
      "Epoch 12/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 15689.6797 - mae: 72.9852 - val_loss: 16176.6377 - val_mae: 72.3196\n",
      "Epoch 13/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 14773.5020 - mae: 68.7326 - val_loss: 14733.6240 - val_mae: 63.3897\n",
      "Epoch 14/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 13462.9326 - mae: 61.2706 - val_loss: 13945.8047 - val_mae: 60.9778\n",
      "Epoch 15/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 12711.0586 - mae: 58.7142 - val_loss: 13217.0283 - val_mae: 58.7273\n",
      "Epoch 16/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 12012.7852 - mae: 56.3677 - val_loss: 12527.1807 - val_mae: 56.4344\n",
      "Epoch 17/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 11359.1309 - mae: 54.2548 - val_loss: 11876.3779 - val_mae: 54.3072\n",
      "Epoch 18/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 10743.8535 - mae: 52.1751 - val_loss: 11271.6377 - val_mae: 52.4350\n",
      "Epoch 19/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 10168.5332 - mae: 50.3788 - val_loss: 10701.9541 - val_mae: 50.8991\n",
      "Epoch 20/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 9625.6719 - mae: 48.6114 - val_loss: 10145.9668 - val_mae: 49.0734\n",
      "Epoch 21/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 9104.8223 - mae: 46.7656 - val_loss: 9650.6934 - val_mae: 48.0209\n",
      "Epoch 22/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 8619.7090 - mae: 45.3115 - val_loss: 9161.7812 - val_mae: 46.6184\n",
      "Epoch 23/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 8156.3062 - mae: 43.7490 - val_loss: 8687.6172 - val_mae: 44.7732\n",
      "Epoch 24/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 7723.6968 - mae: 42.3822 - val_loss: 8235.2266 - val_mae: 43.2934\n",
      "Epoch 25/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 7300.0820 - mae: 40.8829 - val_loss: 7810.1948 - val_mae: 41.9551\n",
      "Epoch 26/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 7019.1528 - mae: 40.1988 - val_loss: 7414.9219 - val_mae: 40.7323\n",
      "Epoch 27/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 6538.9878 - mae: 38.3777 - val_loss: 7024.5435 - val_mae: 39.4655\n",
      "Epoch 28/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 6173.2944 - mae: 36.9432 - val_loss: 6640.3984 - val_mae: 37.9741\n",
      "Epoch 29/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 5835.7788 - mae: 35.8586 - val_loss: 6291.2759 - val_mae: 36.8689\n",
      "Epoch 30/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 5520.5771 - mae: 34.8536 - val_loss: 5961.7329 - val_mae: 35.8226\n",
      "Epoch 31/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 5212.3550 - mae: 33.7438 - val_loss: 5643.0522 - val_mae: 34.8074\n",
      "Epoch 32/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 4921.2979 - mae: 32.6810 - val_loss: 5385.9785 - val_mae: 34.9507\n",
      "Epoch 33/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 4646.6831 - mae: 31.6637 - val_loss: 5076.8896 - val_mae: 33.5176\n",
      "Epoch 34/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 4387.7935 - mae: 30.7752 - val_loss: 4816.1626 - val_mae: 32.9516\n",
      "Epoch 35/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 4145.0269 - mae: 30.0073 - val_loss: 4533.7734 - val_mae: 31.5855\n",
      "Epoch 36/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3913.5432 - mae: 29.2112 - val_loss: 4286.8638 - val_mae: 30.7122\n",
      "Epoch 37/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3689.7024 - mae: 28.2798 - val_loss: 4062.2229 - val_mae: 30.0789\n",
      "Epoch 38/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3487.3562 - mae: 27.7380 - val_loss: 3850.4895 - val_mae: 29.6139\n",
      "Epoch 39/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3290.5818 - mae: 26.9826 - val_loss: 3644.5574 - val_mae: 29.0901\n",
      "Epoch 40/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3099.1484 - mae: 26.1757 - val_loss: 3420.3735 - val_mae: 27.8050\n",
      "Epoch 41/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2924.5117 - mae: 25.5561 - val_loss: 3222.0444 - val_mae: 26.9756\n",
      "Epoch 42/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2770.5881 - mae: 25.1508 - val_loss: 3036.3611 - val_mae: 26.2215\n",
      "Epoch 43/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2605.9597 - mae: 24.3802 - val_loss: 2871.7307 - val_mae: 25.7022\n",
      "Epoch 44/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2456.8955 - mae: 23.8676 - val_loss: 2713.7986 - val_mae: 25.3726\n",
      "Epoch 45/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2315.0764 - mae: 23.2061 - val_loss: 2557.0696 - val_mae: 24.6179\n",
      "Epoch 46/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2176.7849 - mae: 22.5030 - val_loss: 2402.3999 - val_mae: 23.7544\n",
      "Epoch 47/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2066.7737 - mae: 22.3347 - val_loss: 2280.2957 - val_mae: 23.7151\n",
      "Epoch 48/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1942.0482 - mae: 21.7080 - val_loss: 2118.3772 - val_mae: 22.3762\n",
      "Epoch 49/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1829.2227 - mae: 21.2333 - val_loss: 2034.7098 - val_mae: 22.6155\n",
      "Epoch 50/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1730.9159 - mae: 20.8873 - val_loss: 1918.4310 - val_mae: 22.1659\n",
      "Epoch 51/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1629.5101 - mae: 20.3857 - val_loss: 1791.1201 - val_mae: 21.2264\n",
      "Epoch 52/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1552.7617 - mae: 20.2057 - val_loss: 1694.9377 - val_mae: 21.1333\n",
      "Epoch 53/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1466.5752 - mae: 19.8677 - val_loss: 1602.0748 - val_mae: 20.6419\n",
      "Epoch 54/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1370.3328 - mae: 19.1103 - val_loss: 1495.3363 - val_mae: 19.7358\n",
      "Epoch 55/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1306.0748 - mae: 19.0059 - val_loss: 1414.5117 - val_mae: 19.4162\n",
      "Epoch 56/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1233.3967 - mae: 18.5735 - val_loss: 1390.1816 - val_mae: 20.0991\n",
      "Epoch 57/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1168.3412 - mae: 18.3225 - val_loss: 1273.1553 - val_mae: 18.8577\n",
      "Epoch 58/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1100.1342 - mae: 17.8325 - val_loss: 1197.6337 - val_mae: 18.4997\n",
      "Epoch 59/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1046.5560 - mae: 17.7155 - val_loss: 1146.9844 - val_mae: 18.6173\n",
      "Epoch 60/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 987.4801 - mae: 17.3427 - val_loss: 1058.0037 - val_mae: 17.7645\n",
      "Epoch 61/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 949.3530 - mae: 17.3699 - val_loss: 1047.6124 - val_mae: 18.2393\n",
      "Epoch 62/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 901.7294 - mae: 17.0903 - val_loss: 971.6999 - val_mae: 17.5935\n",
      "Epoch 63/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 851.2675 - mae: 16.6564 - val_loss: 914.2835 - val_mae: 17.1534\n",
      "Epoch 64/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 822.3906 - mae: 16.7420 - val_loss: 873.3733 - val_mae: 16.9783\n",
      "Epoch 65/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 784.6539 - mae: 16.4929 - val_loss: 851.5804 - val_mae: 17.0960\n",
      "Epoch 66/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 746.0500 - mae: 16.2648 - val_loss: 803.6740 - val_mae: 16.6255\n",
      "Epoch 67/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 706.2601 - mae: 15.8605 - val_loss: 778.6481 - val_mae: 16.9483\n",
      "Epoch 68/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 686.7906 - mae: 15.9241 - val_loss: 744.2404 - val_mae: 16.6881\n",
      "Epoch 69/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 657.3231 - mae: 15.7565 - val_loss: 714.3650 - val_mae: 16.1675\n",
      "Epoch 70/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 619.4930 - mae: 15.3407 - val_loss: 662.1372 - val_mae: 15.7647\n",
      "Epoch 71/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 612.7659 - mae: 15.5403 - val_loss: 684.8084 - val_mae: 16.8074\n",
      "Epoch 72/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 605.4929 - mae: 15.7210 - val_loss: 630.1155 - val_mae: 16.0437\n",
      "Epoch 73/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 573.6548 - mae: 15.4299 - val_loss: 646.9163 - val_mae: 16.5043\n",
      "Epoch 74/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 556.4143 - mae: 15.2904 - val_loss: 571.9013 - val_mae: 15.4793\n",
      "Epoch 75/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 532.6823 - mae: 15.0415 - val_loss: 544.9698 - val_mae: 15.1423\n",
      "Epoch 76/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 519.2821 - mae: 15.0150 - val_loss: 547.8344 - val_mae: 15.2596\n",
      "Epoch 77/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 501.7603 - mae: 14.8607 - val_loss: 521.8513 - val_mae: 15.0711\n",
      "Epoch 78/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 476.1523 - mae: 14.5575 - val_loss: 489.8852 - val_mae: 14.6000\n",
      "Epoch 79/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 481.9605 - mae: 14.7098 - val_loss: 521.2308 - val_mae: 15.5187\n",
      "Epoch 80/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 455.9849 - mae: 14.4079 - val_loss: 485.3921 - val_mae: 15.0533\n",
      "Epoch 81/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 444.7085 - mae: 14.2785 - val_loss: 480.8395 - val_mae: 14.8877\n",
      "Epoch 82/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 438.2285 - mae: 14.3723 - val_loss: 468.8380 - val_mae: 14.9471\n",
      "Epoch 83/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 443.9525 - mae: 14.5193 - val_loss: 458.8527 - val_mae: 14.8295\n",
      "Epoch 84/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 432.6982 - mae: 14.2853 - val_loss: 448.6204 - val_mae: 14.5990\n",
      "Epoch 85/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 421.5048 - mae: 14.2537 - val_loss: 455.3382 - val_mae: 14.8906\n",
      "Epoch 86/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 411.6335 - mae: 14.2121 - val_loss: 439.1855 - val_mae: 14.8741\n",
      "Epoch 87/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 403.2386 - mae: 14.0545 - val_loss: 394.5349 - val_mae: 13.8932\n",
      "Epoch 88/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 396.1919 - mae: 14.0165 - val_loss: 422.8530 - val_mae: 14.7068\n",
      "Epoch 89/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 393.1755 - mae: 14.0196 - val_loss: 384.1564 - val_mae: 13.9398\n",
      "Epoch 90/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 383.8092 - mae: 13.8456 - val_loss: 413.7520 - val_mae: 14.4625\n",
      "Epoch 91/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 392.1248 - mae: 14.0550 - val_loss: 407.1964 - val_mae: 14.3243\n",
      "Epoch 92/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 377.4292 - mae: 13.8334 - val_loss: 382.8790 - val_mae: 13.8845\n",
      "Epoch 93/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 370.5439 - mae: 13.7988 - val_loss: 395.2108 - val_mae: 14.2331\n",
      "Epoch 94/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 370.8425 - mae: 13.8106 - val_loss: 386.4328 - val_mae: 14.0427\n",
      "Epoch 95/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 372.9335 - mae: 13.8130 - val_loss: 371.2743 - val_mae: 13.9104\n",
      "Epoch 96/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 360.5759 - mae: 13.7781 - val_loss: 362.4646 - val_mae: 13.7600\n",
      "Epoch 97/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 351.4482 - mae: 13.5987 - val_loss: 384.8485 - val_mae: 14.1480\n",
      "Epoch 98/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 368.0464 - mae: 13.8683 - val_loss: 360.5687 - val_mae: 14.0061\n",
      "Epoch 99/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 374.2772 - mae: 14.0483 - val_loss: 381.0874 - val_mae: 14.3258\n",
      "Epoch 100/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 348.7082 - mae: 13.5796 - val_loss: 374.5343 - val_mae: 14.3162\n",
      "Epoch 101/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 352.3145 - mae: 13.6601 - val_loss: 359.7707 - val_mae: 13.9988\n",
      "Epoch 102/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 343.3535 - mae: 13.5387 - val_loss: 365.6211 - val_mae: 13.8592\n",
      "Epoch 103/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 344.6960 - mae: 13.5491 - val_loss: 376.2826 - val_mae: 14.0137\n",
      "Epoch 104/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 345.5946 - mae: 13.6182 - val_loss: 353.3405 - val_mae: 13.8646\n",
      "Epoch 105/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 340.3189 - mae: 13.5487 - val_loss: 347.7643 - val_mae: 13.7168\n",
      "Epoch 106/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 348.1085 - mae: 13.5494 - val_loss: 370.0952 - val_mae: 13.9591\n",
      "Epoch 107/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 343.6660 - mae: 13.5761 - val_loss: 370.0106 - val_mae: 14.2396\n",
      "Epoch 108/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 339.7944 - mae: 13.4655 - val_loss: 347.1276 - val_mae: 13.7760\n",
      "Epoch 109/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 338.4596 - mae: 13.5389 - val_loss: 342.9875 - val_mae: 13.8461\n",
      "Epoch 110/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 342.2995 - mae: 13.5734 - val_loss: 337.1293 - val_mae: 13.4899\n",
      "Epoch 111/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 343.2183 - mae: 13.6760 - val_loss: 328.2999 - val_mae: 13.4845\n",
      "Epoch 112/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 330.7346 - mae: 13.4095 - val_loss: 342.8622 - val_mae: 13.5811\n",
      "Epoch 113/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 336.5066 - mae: 13.5186 - val_loss: 335.6263 - val_mae: 13.6425\n",
      "Epoch 114/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 329.8183 - mae: 13.4602 - val_loss: 325.0560 - val_mae: 13.5150\n",
      "Epoch 115/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 330.0701 - mae: 13.4652 - val_loss: 331.8227 - val_mae: 13.5997\n",
      "Epoch 116/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 332.4138 - mae: 13.4606 - val_loss: 361.7409 - val_mae: 14.1951\n",
      "Epoch 117/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 330.3238 - mae: 13.4965 - val_loss: 324.5559 - val_mae: 13.4590\n",
      "Epoch 118/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 328.7173 - mae: 13.4759 - val_loss: 351.6832 - val_mae: 13.9315\n",
      "Epoch 119/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 328.5481 - mae: 13.5319 - val_loss: 352.7889 - val_mae: 13.9249\n",
      "Epoch 120/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 329.0271 - mae: 13.4396 - val_loss: 339.0002 - val_mae: 13.8591\n",
      "Epoch 121/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 338.8183 - mae: 13.6001 - val_loss: 324.2170 - val_mae: 13.5390\n",
      "Epoch 122/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 324.4408 - mae: 13.4139 - val_loss: 369.6323 - val_mae: 13.9688\n",
      "Epoch 123/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 318.8479 - mae: 13.3185 - val_loss: 327.6334 - val_mae: 13.4702\n",
      "Epoch 124/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 320.7491 - mae: 13.2911 - val_loss: 333.7866 - val_mae: 13.6383\n",
      "Epoch 125/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 319.4474 - mae: 13.3401 - val_loss: 336.7448 - val_mae: 13.6233\n",
      "Epoch 126/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 319.8515 - mae: 13.3601 - val_loss: 341.4689 - val_mae: 13.7566\n",
      "Epoch 127/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 323.5892 - mae: 13.3933 - val_loss: 318.8593 - val_mae: 13.3858\n",
      "Epoch 128/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 325.8810 - mae: 13.4068 - val_loss: 318.8651 - val_mae: 13.3416\n",
      "Epoch 129/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 315.0943 - mae: 13.2468 - val_loss: 326.9243 - val_mae: 13.6098\n",
      "Epoch 130/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 313.7365 - mae: 13.2249 - val_loss: 349.6468 - val_mae: 13.9672\n",
      "Epoch 131/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 316.0325 - mae: 13.2967 - val_loss: 319.7278 - val_mae: 13.4290\n",
      "Epoch 132/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 319.0221 - mae: 13.3802 - val_loss: 325.9367 - val_mae: 13.7303\n",
      "Epoch 133/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 327.6701 - mae: 13.4001 - val_loss: 314.5623 - val_mae: 13.3446\n",
      "Epoch 134/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 314.4162 - mae: 13.2312 - val_loss: 332.7529 - val_mae: 13.5910\n",
      "Epoch 135/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 308.7075 - mae: 13.1248 - val_loss: 339.5231 - val_mae: 13.7260\n",
      "Epoch 136/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 311.4807 - mae: 13.1792 - val_loss: 311.0386 - val_mae: 13.3435\n",
      "Epoch 137/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 309.8916 - mae: 13.2161 - val_loss: 318.2941 - val_mae: 13.4192\n",
      "Epoch 138/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 301.7121 - mae: 12.9493 - val_loss: 316.0744 - val_mae: 13.3259\n",
      "Epoch 139/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 308.4036 - mae: 13.1715 - val_loss: 318.8479 - val_mae: 13.5165\n",
      "Epoch 140/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 304.0618 - mae: 13.0860 - val_loss: 324.0590 - val_mae: 13.3759\n",
      "Epoch 141/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 305.2590 - mae: 13.0458 - val_loss: 318.8533 - val_mae: 13.3696\n",
      "Epoch 142/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 304.5159 - mae: 13.0418 - val_loss: 333.7999 - val_mae: 13.7182\n",
      "Epoch 143/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 312.6352 - mae: 13.1190 - val_loss: 316.8314 - val_mae: 13.2611\n",
      "Epoch 144/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 304.7697 - mae: 13.0641 - val_loss: 311.9875 - val_mae: 13.2943\n",
      "Epoch 145/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 307.4609 - mae: 13.0757 - val_loss: 309.6767 - val_mae: 13.2968\n",
      "Epoch 146/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 311.1764 - mae: 13.1848 - val_loss: 318.9781 - val_mae: 13.4687\n",
      "Epoch 147/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 309.4390 - mae: 13.2299 - val_loss: 321.8356 - val_mae: 13.5073\n",
      "Epoch 148/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 316.1424 - mae: 13.2991 - val_loss: 312.2178 - val_mae: 13.2222\n",
      "Epoch 149/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 320.3630 - mae: 13.4407 - val_loss: 318.2611 - val_mae: 13.3842\n",
      "Epoch 150/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 317.6789 - mae: 13.3381 - val_loss: 320.2200 - val_mae: 13.4857\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 345.7021 - mae: 13.8669\n",
      "Mean Absolute Error on test data: 13.866859436035156\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential([\n",
    "    Dense(512, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    Dense(512, activation='tanh'),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=150, validation_split=0.2)\n",
    "\n",
    "loss, mae = model.evaluate(X_test, y_test)\n",
    "print(\"Mean Absolute Error on test data:\", mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "cc94aa8d-bda0-4512-ae2a-b26682ce825f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step\n",
      "Relative Error on test data: 0.11239430925011241\n"
     ]
    }
   ],
   "source": [
    "# Получение прогнозов модели на тестовых данных\n",
    "y_pred = model.predict(X_test).flatten() \n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error = mae / np.abs(np.mean(y_test))\n",
    "print(\"Relative Error on test data:\", relative_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "6951af4b-4b43-465b-b99d-1a5ebdb63953",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:At this time, the v2.11+ optimizer `tf.keras.optimizers.Adam` runs slowly on M1/M2 Macs, please use the legacy Keras optimizer instead, located at `tf.keras.optimizers.legacy.Adam`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "257/257 [==============================] - 3s 10ms/step - loss: 29592.7441 - mae: 119.7636 - val_loss: 29204.4062 - val_mae: 114.8651\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 27313.0586 - mae: 110.5033 - val_loss: 26997.0020 - val_mae: 106.3210\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 24840.8750 - mae: 101.6952 - val_loss: 24503.3359 - val_mae: 97.7867\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 22598.0762 - mae: 94.7175 - val_loss: 22267.9629 - val_mae: 91.7918\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 20451.5137 - mae: 88.7515 - val_loss: 19889.8262 - val_mae: 81.0189\n",
      "Epoch 6/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 18098.1914 - mae: 78.8743 - val_loss: 17586.6133 - val_mae: 72.6085\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 16184.0215 - mae: 72.5343 - val_loss: 15558.8418 - val_mae: 66.1159\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 14379.4082 - mae: 67.2075 - val_loss: 13708.0918 - val_mae: 60.2468\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 12609.3271 - mae: 62.0348 - val_loss: 12041.4141 - val_mae: 55.2268\n",
      "Epoch 10/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 11153.7695 - mae: 57.9893 - val_loss: 10587.8418 - val_mae: 51.3967\n",
      "Epoch 11/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 9867.6660 - mae: 54.2058 - val_loss: 9269.7695 - val_mae: 46.7657\n",
      "Epoch 12/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 8869.0918 - mae: 51.0222 - val_loss: 8164.1567 - val_mae: 44.2209\n",
      "Epoch 13/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 7967.5703 - mae: 48.9356 - val_loss: 7114.1924 - val_mae: 40.2179\n",
      "Epoch 14/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 7163.9746 - mae: 46.4084 - val_loss: 6245.8721 - val_mae: 37.4826\n",
      "Epoch 15/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 6669.1870 - mae: 45.0301 - val_loss: 5516.1953 - val_mae: 35.6064\n",
      "Epoch 16/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 5848.5933 - mae: 42.6967 - val_loss: 4899.2754 - val_mae: 34.9427\n",
      "Epoch 17/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 5471.9897 - mae: 41.9454 - val_loss: 4252.1562 - val_mae: 31.2484\n",
      "Epoch 18/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 5073.9263 - mae: 40.5752 - val_loss: 3783.1204 - val_mae: 29.9987\n",
      "Epoch 19/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 4945.4375 - mae: 40.5418 - val_loss: 3330.4360 - val_mae: 27.9689\n",
      "Epoch 20/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 4472.3877 - mae: 38.9270 - val_loss: 2980.8604 - val_mae: 27.0367\n",
      "Epoch 21/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 4229.3936 - mae: 38.5141 - val_loss: 2680.5181 - val_mae: 25.8278\n",
      "Epoch 22/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3904.9727 - mae: 36.9084 - val_loss: 2502.4075 - val_mae: 26.7853\n",
      "Epoch 23/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3752.8037 - mae: 36.6963 - val_loss: 2319.8613 - val_mae: 26.7609\n",
      "Epoch 24/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3704.6257 - mae: 36.3609 - val_loss: 1991.0820 - val_mae: 23.0978\n",
      "Epoch 25/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3546.0386 - mae: 35.8253 - val_loss: 1938.5626 - val_mae: 24.4526\n",
      "Epoch 26/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3413.3831 - mae: 35.8252 - val_loss: 1661.2045 - val_mae: 21.7712\n",
      "Epoch 27/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3392.1755 - mae: 35.6115 - val_loss: 1591.6132 - val_mae: 22.2910\n",
      "Epoch 28/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3348.1758 - mae: 35.3104 - val_loss: 1425.6710 - val_mae: 20.7669\n",
      "Epoch 29/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3343.1140 - mae: 35.4649 - val_loss: 1458.2849 - val_mae: 22.3958\n",
      "Epoch 30/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3221.5388 - mae: 35.1161 - val_loss: 1245.9536 - val_mae: 19.8773\n",
      "Epoch 31/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3169.0818 - mae: 34.8653 - val_loss: 1243.3654 - val_mae: 20.4228\n",
      "Epoch 32/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3079.4255 - mae: 34.8312 - val_loss: 1247.4797 - val_mae: 20.8886\n",
      "Epoch 33/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3268.2861 - mae: 35.3237 - val_loss: 1194.7025 - val_mae: 20.8639\n",
      "Epoch 34/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3050.6785 - mae: 34.4866 - val_loss: 1080.1240 - val_mae: 19.6439\n",
      "Epoch 35/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3062.7500 - mae: 34.6142 - val_loss: 1043.1757 - val_mae: 19.6105\n",
      "Epoch 36/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3112.1724 - mae: 34.8242 - val_loss: 1012.9600 - val_mae: 19.3801\n",
      "Epoch 37/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2919.3794 - mae: 34.1948 - val_loss: 952.7849 - val_mae: 18.9474\n",
      "Epoch 38/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2843.1482 - mae: 33.6862 - val_loss: 955.7913 - val_mae: 19.1057\n",
      "Epoch 39/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2887.1042 - mae: 33.8345 - val_loss: 865.9822 - val_mae: 18.1786\n",
      "Epoch 40/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2984.1790 - mae: 34.2259 - val_loss: 905.0573 - val_mae: 18.7462\n",
      "Epoch 41/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 3003.2139 - mae: 33.9696 - val_loss: 872.3912 - val_mae: 18.8159\n",
      "Epoch 42/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2932.8767 - mae: 33.8514 - val_loss: 825.1341 - val_mae: 18.1977\n",
      "Epoch 43/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2982.4663 - mae: 34.0777 - val_loss: 793.9839 - val_mae: 17.7027\n",
      "Epoch 44/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2911.4126 - mae: 33.9317 - val_loss: 801.7137 - val_mae: 18.2647\n",
      "Epoch 45/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2944.9031 - mae: 34.1103 - val_loss: 745.7437 - val_mae: 17.4606\n",
      "Epoch 46/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2841.0515 - mae: 33.5065 - val_loss: 793.8271 - val_mae: 18.7776\n",
      "Epoch 47/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2979.0464 - mae: 34.2867 - val_loss: 807.5503 - val_mae: 18.9844\n",
      "Epoch 48/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2750.8457 - mae: 33.3599 - val_loss: 698.6661 - val_mae: 17.1992\n",
      "Epoch 49/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2926.7717 - mae: 33.6749 - val_loss: 760.0947 - val_mae: 18.2126\n",
      "Epoch 50/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2691.1697 - mae: 32.9670 - val_loss: 708.5352 - val_mae: 17.6714\n",
      "Epoch 51/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2693.3501 - mae: 32.8667 - val_loss: 781.8564 - val_mae: 19.2212\n",
      "Epoch 52/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2746.4019 - mae: 32.8813 - val_loss: 775.9736 - val_mae: 19.6745\n",
      "Epoch 53/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2791.5129 - mae: 33.6545 - val_loss: 758.8468 - val_mae: 18.7135\n",
      "Epoch 54/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2866.1704 - mae: 33.4153 - val_loss: 720.2692 - val_mae: 18.1012\n",
      "Epoch 55/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2860.5173 - mae: 33.8124 - val_loss: 666.0982 - val_mae: 17.9749\n",
      "Epoch 56/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2766.2964 - mae: 33.3056 - val_loss: 656.4372 - val_mae: 17.0669\n",
      "Epoch 57/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2864.9622 - mae: 33.5082 - val_loss: 662.4028 - val_mae: 17.5568\n",
      "Epoch 58/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2833.4592 - mae: 33.4624 - val_loss: 935.6486 - val_mae: 23.6103\n",
      "Epoch 59/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2756.4612 - mae: 33.2661 - val_loss: 652.4672 - val_mae: 17.3763\n",
      "Epoch 60/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2747.3601 - mae: 33.1813 - val_loss: 635.4540 - val_mae: 17.0105\n",
      "Epoch 61/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2788.0132 - mae: 33.4496 - val_loss: 643.4431 - val_mae: 17.1561\n",
      "Epoch 62/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2655.2432 - mae: 32.8116 - val_loss: 632.9416 - val_mae: 17.0262\n",
      "Epoch 63/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2665.9646 - mae: 33.0517 - val_loss: 687.8885 - val_mae: 17.9910\n",
      "Epoch 64/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2651.9929 - mae: 32.6640 - val_loss: 651.6885 - val_mae: 17.3658\n",
      "Epoch 65/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2533.8030 - mae: 32.2527 - val_loss: 698.5001 - val_mae: 19.0565\n",
      "Epoch 66/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2609.6541 - mae: 32.8577 - val_loss: 649.5555 - val_mae: 17.7725\n",
      "Epoch 67/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2549.3420 - mae: 32.3492 - val_loss: 568.0521 - val_mae: 16.1852\n",
      "Epoch 68/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2592.9644 - mae: 32.4758 - val_loss: 589.3610 - val_mae: 16.4592\n",
      "Epoch 69/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2823.7971 - mae: 33.3764 - val_loss: 580.0222 - val_mae: 16.5336\n",
      "Epoch 70/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2672.1125 - mae: 32.5937 - val_loss: 591.1448 - val_mae: 17.0569\n",
      "Epoch 71/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2633.1082 - mae: 32.5567 - val_loss: 713.0943 - val_mae: 18.8145\n",
      "Epoch 72/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2632.3240 - mae: 32.4019 - val_loss: 673.8423 - val_mae: 18.3661\n",
      "Epoch 73/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2654.5823 - mae: 32.7684 - val_loss: 575.5544 - val_mae: 17.0463\n",
      "Epoch 74/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2712.1631 - mae: 32.8412 - val_loss: 758.6057 - val_mae: 20.6665\n",
      "Epoch 75/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2675.1313 - mae: 32.9042 - val_loss: 519.3548 - val_mae: 15.7928\n",
      "Epoch 76/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2423.1724 - mae: 31.8681 - val_loss: 680.7217 - val_mae: 18.7530\n",
      "Epoch 77/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2582.5664 - mae: 32.2718 - val_loss: 685.2496 - val_mae: 18.6812\n",
      "Epoch 78/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2649.9187 - mae: 32.4660 - val_loss: 546.0873 - val_mae: 16.3630\n",
      "Epoch 79/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2618.6099 - mae: 32.5074 - val_loss: 662.4366 - val_mae: 18.7198\n",
      "Epoch 80/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2545.4138 - mae: 32.4887 - val_loss: 539.0026 - val_mae: 16.2408\n",
      "Epoch 81/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2565.8284 - mae: 32.3323 - val_loss: 550.7031 - val_mae: 16.6241\n",
      "Epoch 82/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2529.7786 - mae: 32.0215 - val_loss: 525.8099 - val_mae: 16.0922\n",
      "Epoch 83/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2711.3586 - mae: 32.9007 - val_loss: 602.9273 - val_mae: 17.5322\n",
      "Epoch 84/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2507.7539 - mae: 31.8260 - val_loss: 529.9180 - val_mae: 16.0972\n",
      "Epoch 85/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2376.9851 - mae: 31.3398 - val_loss: 534.4943 - val_mae: 16.1580\n",
      "Epoch 86/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2489.6182 - mae: 31.8791 - val_loss: 637.3611 - val_mae: 18.4021\n",
      "Epoch 87/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2482.2295 - mae: 31.7377 - val_loss: 572.9902 - val_mae: 16.9842\n",
      "Epoch 88/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2618.2710 - mae: 32.3950 - val_loss: 546.6116 - val_mae: 16.6771\n",
      "Epoch 89/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2589.0432 - mae: 32.5046 - val_loss: 584.4543 - val_mae: 16.9097\n",
      "Epoch 90/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2610.2544 - mae: 32.6979 - val_loss: 559.9599 - val_mae: 16.8213\n",
      "Epoch 91/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2561.0493 - mae: 32.1938 - val_loss: 473.1786 - val_mae: 15.3632\n",
      "Epoch 92/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2696.9519 - mae: 32.8925 - val_loss: 563.7106 - val_mae: 16.8686\n",
      "Epoch 93/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2629.9602 - mae: 32.6605 - val_loss: 468.3918 - val_mae: 15.2809\n",
      "Epoch 94/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2508.8052 - mae: 32.0928 - val_loss: 540.4523 - val_mae: 16.5098\n",
      "Epoch 95/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2562.9807 - mae: 32.1205 - val_loss: 540.5688 - val_mae: 16.5880\n",
      "Epoch 96/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2644.4731 - mae: 32.6174 - val_loss: 534.3450 - val_mae: 16.3372\n",
      "Epoch 97/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2532.5967 - mae: 32.1406 - val_loss: 528.8604 - val_mae: 16.2704\n",
      "Epoch 98/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2393.5149 - mae: 31.7584 - val_loss: 520.0663 - val_mae: 16.2099\n",
      "Epoch 99/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2461.3562 - mae: 31.7478 - val_loss: 593.3640 - val_mae: 17.9415\n",
      "Epoch 100/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2603.3215 - mae: 32.1853 - val_loss: 488.7364 - val_mae: 15.8439\n",
      "Epoch 101/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2483.7192 - mae: 31.9543 - val_loss: 539.1995 - val_mae: 16.6418\n",
      "Epoch 102/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2424.7539 - mae: 31.9249 - val_loss: 522.4434 - val_mae: 16.2031\n",
      "Epoch 103/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2510.6760 - mae: 31.8472 - val_loss: 455.5823 - val_mae: 15.1923\n",
      "Epoch 104/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2597.3467 - mae: 32.0569 - val_loss: 466.2071 - val_mae: 15.3560\n",
      "Epoch 105/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2505.6370 - mae: 31.8183 - val_loss: 444.5827 - val_mae: 15.0682\n",
      "Epoch 106/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2598.1465 - mae: 32.1114 - val_loss: 556.3199 - val_mae: 17.3440\n",
      "Epoch 107/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2505.1421 - mae: 31.8122 - val_loss: 522.8796 - val_mae: 16.4729\n",
      "Epoch 108/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2605.7163 - mae: 32.0181 - val_loss: 511.5722 - val_mae: 16.3660\n",
      "Epoch 109/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2437.0562 - mae: 31.7969 - val_loss: 503.1793 - val_mae: 16.3790\n",
      "Epoch 110/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2473.1230 - mae: 31.8543 - val_loss: 483.9057 - val_mae: 15.7945\n",
      "Epoch 111/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2492.2935 - mae: 32.0656 - val_loss: 585.4373 - val_mae: 17.4162\n",
      "Epoch 112/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2600.9829 - mae: 32.4360 - val_loss: 441.9159 - val_mae: 15.1436\n",
      "Epoch 113/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2485.7800 - mae: 31.9703 - val_loss: 475.2833 - val_mae: 15.7744\n",
      "Epoch 114/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2562.2151 - mae: 32.1197 - val_loss: 461.4037 - val_mae: 15.4724\n",
      "Epoch 115/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2482.8948 - mae: 31.7746 - val_loss: 530.7512 - val_mae: 16.4160\n",
      "Epoch 116/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2443.3975 - mae: 31.7120 - val_loss: 512.0532 - val_mae: 16.2934\n",
      "Epoch 117/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2473.8796 - mae: 31.5765 - val_loss: 512.1406 - val_mae: 16.2864\n",
      "Epoch 118/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2472.2500 - mae: 31.5457 - val_loss: 484.1400 - val_mae: 15.9639\n",
      "Epoch 119/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2420.4202 - mae: 31.4738 - val_loss: 489.2988 - val_mae: 15.8733\n",
      "Epoch 120/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2523.5479 - mae: 32.2327 - val_loss: 443.2872 - val_mae: 15.0904\n",
      "Epoch 121/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2410.2866 - mae: 31.0371 - val_loss: 466.1245 - val_mae: 15.5710\n",
      "Epoch 122/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2540.1531 - mae: 31.8499 - val_loss: 507.4639 - val_mae: 16.5846\n",
      "Epoch 123/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2423.3928 - mae: 31.3327 - val_loss: 496.0231 - val_mae: 16.0076\n",
      "Epoch 124/150\n",
      "257/257 [==============================] - 2s 8ms/step - loss: 2388.7441 - mae: 31.5928 - val_loss: 496.3762 - val_mae: 16.3566\n",
      "Epoch 125/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2389.6338 - mae: 31.6741 - val_loss: 478.4781 - val_mae: 15.8461\n",
      "Epoch 126/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2375.3176 - mae: 31.2990 - val_loss: 474.1544 - val_mae: 15.6619\n",
      "Epoch 127/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2579.0066 - mae: 32.2062 - val_loss: 499.7235 - val_mae: 15.9623\n",
      "Epoch 128/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2624.0977 - mae: 32.2104 - val_loss: 618.1370 - val_mae: 18.4247\n",
      "Epoch 129/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2511.1379 - mae: 31.8629 - val_loss: 508.8808 - val_mae: 16.2177\n",
      "Epoch 130/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2577.3284 - mae: 32.1244 - val_loss: 422.2751 - val_mae: 14.8330\n",
      "Epoch 131/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2419.7209 - mae: 31.4236 - val_loss: 422.2466 - val_mae: 14.8192\n",
      "Epoch 132/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2332.0105 - mae: 31.3318 - val_loss: 434.7238 - val_mae: 15.0340\n",
      "Epoch 133/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2415.4709 - mae: 31.6556 - val_loss: 540.0442 - val_mae: 16.7272\n",
      "Epoch 134/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2434.6169 - mae: 31.6105 - val_loss: 484.6843 - val_mae: 15.7760\n",
      "Epoch 135/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2590.6995 - mae: 32.2063 - val_loss: 448.3991 - val_mae: 15.2903\n",
      "Epoch 136/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2454.9006 - mae: 31.5870 - val_loss: 519.9046 - val_mae: 16.9798\n",
      "Epoch 137/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2351.5837 - mae: 31.3382 - val_loss: 578.4771 - val_mae: 17.2474\n",
      "Epoch 138/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2390.7051 - mae: 31.5427 - val_loss: 448.1201 - val_mae: 15.3312\n",
      "Epoch 139/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2422.5623 - mae: 31.4284 - val_loss: 455.2540 - val_mae: 15.4816\n",
      "Epoch 140/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2463.7717 - mae: 31.7202 - val_loss: 490.1231 - val_mae: 16.2480\n",
      "Epoch 141/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2460.0747 - mae: 31.6521 - val_loss: 465.3542 - val_mae: 15.5565\n",
      "Epoch 142/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2426.1816 - mae: 31.8735 - val_loss: 452.8325 - val_mae: 15.3847\n",
      "Epoch 143/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2434.0715 - mae: 31.4855 - val_loss: 458.2681 - val_mae: 15.4468\n",
      "Epoch 144/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2476.6462 - mae: 31.6469 - val_loss: 404.0145 - val_mae: 14.6862\n",
      "Epoch 145/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2460.7288 - mae: 31.8088 - val_loss: 524.5656 - val_mae: 16.5306\n",
      "Epoch 146/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2481.7180 - mae: 31.7283 - val_loss: 480.6505 - val_mae: 16.2309\n",
      "Epoch 147/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2470.1201 - mae: 31.7257 - val_loss: 451.3509 - val_mae: 15.4427\n",
      "Epoch 148/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2484.6309 - mae: 31.9849 - val_loss: 443.9025 - val_mae: 15.2941\n",
      "Epoch 149/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2426.9583 - mae: 31.4652 - val_loss: 460.5177 - val_mae: 15.7760\n",
      "Epoch 150/150\n",
      "257/257 [==============================] - 2s 9ms/step - loss: 2455.0417 - mae: 31.8407 - val_loss: 486.6813 - val_mae: 16.0492\n",
      "81/81 [==============================] - 0s 2ms/step - loss: 522.6926 - mae: 16.7180\n",
      "Mean Absolute Error on test data: 16.718029022216797\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model_0 = Sequential([\n",
    "    Dense(512, activation='tanh', kernel_regularizer=regularizers.l2(0.01), input_shape=(X_0_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "model_0.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss='mse', metrics=['mae'])\n",
    "\n",
    "history_0 = model_0.fit(X_0_train, y_0_train, epochs=150, validation_split=0.2)\n",
    "\n",
    "loss_0, mae_0 = model_0.evaluate(X_0_test, y_0_test)\n",
    "print(\"Mean Absolute Error on test data:\", mae_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d099a815-5f56-430b-ba79-d9b9bd4e034d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step\n",
      "Relative Error on test data: 0.13550374060131384\n"
     ]
    }
   ],
   "source": [
    "# Получение прогнозов модели на тестовых данных\n",
    "y_pred_0 = model_0.predict(X_0_test).flatten() \n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error_0 = mae_0 / np.abs(np.mean(y_0_test))\n",
    "print(\"Relative Error on test data:\", relative_error_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5780e6e5-1e18-41d1-a85f-adb1e10720a5",
   "metadata": {},
   "source": [
    "## Обучение моделей для предсказания параметра 'Delay'\n",
    "Далее, разделим данные на X - матрица объекты-признаки и y - указания учителя  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3af69d33-f2f5-434c-9a6f-562bc5ea1082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Данные, для которых будем проводить нормализацию\n",
    "X1 = tmp\n",
    "# Данные, для которых не будем проводить нормализацию\n",
    "X1_0 = tmp\n",
    "#Целевые параметры. Разделим их, для возможных экспериментов с ними в будущем\n",
    "y1 = data['Area']\n",
    "y1_0 = data['Area']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3882583a-8da1-4ed0-bee8-0b31caca405e",
   "metadata": {},
   "source": [
    "Проведем нормализацию данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "f970b1b9-3de6-4462-95fa-344505e74da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X1_0 = scaler.fit_transform(X1_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f26fa00-71b9-4b6f-a10a-08acb23f472b",
   "metadata": {},
   "source": [
    "Разделим данные на тренировочные и тестовые"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "ed671a85-b4b2-4fad-8878-7d9a9d1c419d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.20, random_state=42)\n",
    "\n",
    "X1_0_train, X1_0_test, y1_0_train, y1_0_test = train_test_split(X1_0, y1_0, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f5e337-a54b-4efc-a06d-4cb3986363b4",
   "metadata": {},
   "source": [
    "### Выбор, настройка и обучение моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4db3d87-640f-4d75-8413-ed341789ccb5",
   "metadata": {},
   "source": [
    "### Градиентный бустинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "aaae335e-5854-4f61-bfaa-9a5b3d58cca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9817962040034501\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_regressor1 = GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                         n_estimators=500, \n",
    "                                         max_features='sqrt', \n",
    "                                         random_state=10)\n",
    "gb_regressor1.fit(X1_train, y1_train)\n",
    "\n",
    "# Оценка качества модели на тестовых данных\n",
    "accuracy1 = gb_regressor1.score(X1_test, y1_test)\n",
    "print(\"Accuracy:\", accuracy1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d66dc7c6-a248-474b-808a-212af942525f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9818564715830892\n"
     ]
    }
   ],
   "source": [
    "#Нормализованные данные\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb_regressor1_0 = GradientBoostingRegressor(learning_rate=0.01, \n",
    "                                         n_estimators=500, \n",
    "                                         max_features='sqrt', \n",
    "                                         random_state=10)\n",
    "gb_regressor1_0.fit(X1_0_train, y1_0_train)\n",
    "\n",
    "# Оценка качества модели на тестовых данных\n",
    "accuracy1_0 = gb_regressor1_0.score(X1_0_test, y1_0_test)\n",
    "print(\"Accuracy:\", accuracy1_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb09a0b-b2d1-4f20-8fa1-daa8ce294441",
   "metadata": {},
   "source": [
    "### Нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "26a3b95a-bd5c-490b-8cbf-f5bd6dd88e44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 28272.0625 - mae: 114.2931 - val_loss: 28291.4336 - val_mae: 111.3018\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 26416.4316 - mae: 107.0080 - val_loss: 26576.7109 - val_mae: 104.8557\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 24820.3516 - mae: 101.1851 - val_loss: 25060.2949 - val_mae: 99.6302\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 23405.4902 - mae: 96.5528 - val_loss: 23720.6543 - val_mae: 95.5530\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 22147.9102 - mae: 92.9938 - val_loss: 22517.1895 - val_mae: 92.4307\n",
      "Epoch 6/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 21037.6621 - mae: 90.2830 - val_loss: 21466.6777 - val_mae: 90.0563\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 20063.9746 - mae: 88.4116 - val_loss: 20542.0273 - val_mae: 88.3221\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 19211.8086 - mae: 87.1078 - val_loss: 19739.1348 - val_mae: 87.1685\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 18472.2930 - mae: 86.3367 - val_loss: 19041.5117 - val_mae: 86.5024\n",
      "Epoch 10/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 17836.6797 - mae: 86.0442 - val_loss: 18442.4688 - val_mae: 86.3063\n",
      "Epoch 11/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 17298.7168 - mae: 86.0517 - val_loss: 17935.5547 - val_mae: 86.4358\n",
      "Epoch 12/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 16846.1113 - mae: 86.2857 - val_loss: 17516.3027 - val_mae: 86.7820\n",
      "Epoch 13/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 16470.2422 - mae: 86.7235 - val_loss: 17165.9609 - val_mae: 87.3007\n",
      "Epoch 14/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 16165.1416 - mae: 87.3296 - val_loss: 16882.7715 - val_mae: 87.9666\n",
      "Epoch 15/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15921.7812 - mae: 87.9860 - val_loss: 16662.6035 - val_mae: 88.7180\n",
      "Epoch 16/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15732.3369 - mae: 88.7312 - val_loss: 16487.4277 - val_mae: 89.5246\n",
      "Epoch 17/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15587.6689 - mae: 89.4483 - val_loss: 16358.2354 - val_mae: 90.3162\n",
      "Epoch 18/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15479.6611 - mae: 90.1379 - val_loss: 16260.0391 - val_mae: 91.0991\n",
      "Epoch 19/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15402.3633 - mae: 90.8956 - val_loss: 16190.1953 - val_mae: 91.8278\n",
      "Epoch 20/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 15349.0840 - mae: 91.5324 - val_loss: 16142.8154 - val_mae: 92.4743\n",
      "Epoch 21/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15313.3740 - mae: 92.0342 - val_loss: 16109.8086 - val_mae: 93.0639\n",
      "Epoch 22/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 15291.2666 - mae: 92.6864 - val_loss: 16089.9971 - val_mae: 93.5188\n",
      "Epoch 23/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 15277.3672 - mae: 92.9719 - val_loss: 16076.8916 - val_mae: 93.9084\n",
      "Epoch 24/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15268.9941 - mae: 93.2886 - val_loss: 16068.2686 - val_mae: 94.2389\n",
      "Epoch 25/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15264.0576 - mae: 93.5710 - val_loss: 16062.9795 - val_mae: 94.5016\n",
      "Epoch 26/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15261.3574 - mae: 93.7155 - val_loss: 16060.0615 - val_mae: 94.6879\n",
      "Epoch 27/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15259.9824 - mae: 93.9292 - val_loss: 16058.8926 - val_mae: 94.7770\n",
      "Epoch 28/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15259.2393 - mae: 93.9995 - val_loss: 16057.4014 - val_mae: 94.9144\n",
      "Epoch 29/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15258.8613 - mae: 94.1310 - val_loss: 16056.7236 - val_mae: 94.9905\n",
      "Epoch 30/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 15258.8154 - mae: 94.1764 - val_loss: 16056.6299 - val_mae: 94.9982\n",
      "Epoch 31/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 14516.7041 - mae: 87.4404 - val_loss: 12306.6191 - val_mae: 61.9665\n",
      "Epoch 32/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 10693.1035 - mae: 55.7742 - val_loss: 10989.3359 - val_mae: 55.6898\n",
      "Epoch 33/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 9654.2480 - mae: 50.8351 - val_loss: 10035.4990 - val_mae: 50.4948\n",
      "Epoch 34/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 8870.2656 - mae: 47.5242 - val_loss: 9309.4502 - val_mae: 47.9652\n",
      "Epoch 35/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 8238.6055 - mae: 45.2817 - val_loss: 8699.3936 - val_mae: 46.3228\n",
      "Epoch 36/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 7717.7373 - mae: 43.8821 - val_loss: 8169.1528 - val_mae: 44.2339\n",
      "Epoch 37/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 7253.9033 - mae: 42.7618 - val_loss: 7673.2183 - val_mae: 42.3735\n",
      "Epoch 38/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 6782.1509 - mae: 40.6657 - val_loss: 7218.1631 - val_mae: 41.0164\n",
      "Epoch 39/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 6367.2559 - mae: 39.0144 - val_loss: 6929.6548 - val_mae: 40.2134\n",
      "Epoch 40/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 5993.6201 - mae: 37.6737 - val_loss: 6429.3159 - val_mae: 38.2273\n",
      "Epoch 41/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 5649.9248 - mae: 36.4696 - val_loss: 6103.6675 - val_mae: 37.8292\n",
      "Epoch 42/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 5332.8335 - mae: 35.2923 - val_loss: 5800.4829 - val_mae: 37.3013\n",
      "Epoch 43/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 5039.1367 - mae: 34.5839 - val_loss: 5517.2334 - val_mae: 36.8203\n",
      "Epoch 44/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 4769.1006 - mae: 34.0086 - val_loss: 5172.4146 - val_mae: 34.9697\n",
      "Epoch 45/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 4478.8613 - mae: 32.4521 - val_loss: 4875.5122 - val_mae: 33.8421\n",
      "Epoch 46/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 4223.5376 - mae: 31.3596 - val_loss: 4653.2690 - val_mae: 33.5797\n",
      "Epoch 47/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3993.8462 - mae: 30.6197 - val_loss: 4368.0957 - val_mae: 31.9567\n",
      "Epoch 48/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3790.1602 - mae: 29.9228 - val_loss: 4134.7842 - val_mae: 31.1255\n",
      "Epoch 49/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3588.4717 - mae: 29.4689 - val_loss: 3970.3218 - val_mae: 32.0866\n",
      "Epoch 50/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3372.9299 - mae: 28.5699 - val_loss: 3682.6394 - val_mae: 29.4074\n",
      "Epoch 51/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3190.5286 - mae: 27.8342 - val_loss: 3498.2444 - val_mae: 29.2317\n",
      "Epoch 52/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 3027.6606 - mae: 27.5362 - val_loss: 3287.8103 - val_mae: 28.4635\n",
      "Epoch 53/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 2842.7280 - mae: 26.5889 - val_loss: 3108.0630 - val_mae: 27.8314\n",
      "Epoch 54/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 2690.0042 - mae: 26.0624 - val_loss: 2941.9326 - val_mae: 26.7177\n",
      "Epoch 55/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 2538.7358 - mae: 25.4264 - val_loss: 2787.3298 - val_mae: 26.3364\n",
      "Epoch 56/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 2448.5039 - mae: 25.3880 - val_loss: 2677.4490 - val_mae: 26.5025\n",
      "Epoch 57/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 2280.7710 - mae: 24.4048 - val_loss: 2486.7200 - val_mae: 25.4254\n",
      "Epoch 58/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2131.2207 - mae: 23.4941 - val_loss: 2334.8823 - val_mae: 24.2736\n",
      "Epoch 59/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2018.4403 - mae: 22.9594 - val_loss: 2231.8779 - val_mae: 24.1266\n",
      "Epoch 60/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1912.4659 - mae: 22.5105 - val_loss: 2097.0859 - val_mae: 23.6240\n",
      "Epoch 61/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1895.1420 - mae: 22.5498 - val_loss: 2047.1853 - val_mae: 24.4786\n",
      "Epoch 62/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1745.4064 - mae: 22.4413 - val_loss: 1877.1713 - val_mae: 22.8149\n",
      "Epoch 63/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1633.2906 - mae: 21.4804 - val_loss: 1784.8859 - val_mae: 22.8786\n",
      "Epoch 64/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1544.1018 - mae: 21.1812 - val_loss: 1677.8618 - val_mae: 21.9306\n",
      "Epoch 65/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1463.6144 - mae: 20.8393 - val_loss: 1604.0714 - val_mae: 22.0823\n",
      "Epoch 66/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1382.5665 - mae: 20.3944 - val_loss: 1531.7013 - val_mae: 21.5600\n",
      "Epoch 67/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 1304.8325 - mae: 19.9988 - val_loss: 1439.4563 - val_mae: 20.9350\n",
      "Epoch 68/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1243.7100 - mae: 19.6822 - val_loss: 1317.6693 - val_mae: 19.6864\n",
      "Epoch 69/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1209.9938 - mae: 19.6943 - val_loss: 1352.1072 - val_mae: 20.6962\n",
      "Epoch 70/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1163.5319 - mae: 19.5281 - val_loss: 1278.2820 - val_mae: 20.3789\n",
      "Epoch 71/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1086.0493 - mae: 18.8898 - val_loss: 1187.0031 - val_mae: 19.8284\n",
      "Epoch 72/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1031.5593 - mae: 18.6573 - val_loss: 1122.8007 - val_mae: 19.3122\n",
      "Epoch 73/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 990.5250 - mae: 18.5235 - val_loss: 1039.2476 - val_mae: 18.4133\n",
      "Epoch 74/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 939.1403 - mae: 18.1341 - val_loss: 1023.4260 - val_mae: 18.7962\n",
      "Epoch 75/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 910.8683 - mae: 18.1266 - val_loss: 980.9836 - val_mae: 18.5811\n",
      "Epoch 76/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 871.8964 - mae: 17.9288 - val_loss: 941.8065 - val_mae: 18.4583\n",
      "Epoch 77/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 848.7258 - mae: 17.8846 - val_loss: 902.1104 - val_mae: 18.2449\n",
      "Epoch 78/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 808.8171 - mae: 17.4864 - val_loss: 904.2111 - val_mae: 18.4643\n",
      "Epoch 79/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 790.2375 - mae: 17.5893 - val_loss: 818.9863 - val_mae: 17.6541\n",
      "Epoch 80/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 737.1564 - mae: 17.0934 - val_loss: 785.5560 - val_mae: 17.4875\n",
      "Epoch 81/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 703.6992 - mae: 16.8254 - val_loss: 762.3802 - val_mae: 17.4529\n",
      "Epoch 82/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 688.4916 - mae: 16.7923 - val_loss: 726.6465 - val_mae: 17.1246\n",
      "Epoch 83/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 664.0637 - mae: 16.8205 - val_loss: 696.6469 - val_mae: 16.8881\n",
      "Epoch 84/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 658.0095 - mae: 16.9124 - val_loss: 706.5960 - val_mae: 17.0424\n",
      "Epoch 85/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 632.9382 - mae: 16.5454 - val_loss: 647.4286 - val_mae: 16.3793\n",
      "Epoch 86/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 750.5621 - mae: 17.3815 - val_loss: 847.4845 - val_mae: 18.0423\n",
      "Epoch 87/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 640.7075 - mae: 16.6572 - val_loss: 623.4698 - val_mae: 16.6252\n",
      "Epoch 88/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 581.6801 - mae: 16.2587 - val_loss: 605.3187 - val_mae: 16.3684\n",
      "Epoch 89/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 560.7488 - mae: 16.0918 - val_loss: 601.4070 - val_mae: 16.9546\n",
      "Epoch 90/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 547.2733 - mae: 16.0764 - val_loss: 609.7083 - val_mae: 16.8706\n",
      "Epoch 91/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 605.0516 - mae: 16.6532 - val_loss: 560.7601 - val_mae: 16.1878\n",
      "Epoch 92/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 529.0801 - mae: 15.8857 - val_loss: 518.5859 - val_mae: 15.7662\n",
      "Epoch 93/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 513.8444 - mae: 15.8016 - val_loss: 520.0806 - val_mae: 15.6905\n",
      "Epoch 94/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 509.7165 - mae: 15.7971 - val_loss: 545.1697 - val_mae: 16.4696\n",
      "Epoch 95/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 501.4182 - mae: 15.6322 - val_loss: 516.9251 - val_mae: 15.7688\n",
      "Epoch 96/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 494.3514 - mae: 15.7975 - val_loss: 510.6262 - val_mae: 15.9510\n",
      "Epoch 97/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 480.2017 - mae: 15.6552 - val_loss: 503.7218 - val_mae: 15.9133\n",
      "Epoch 98/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 475.4524 - mae: 15.5357 - val_loss: 534.4932 - val_mae: 16.0921\n",
      "Epoch 99/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 469.7527 - mae: 15.4568 - val_loss: 474.4545 - val_mae: 15.7419\n",
      "Epoch 100/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 490.5823 - mae: 15.8404 - val_loss: 578.5903 - val_mae: 16.8665\n",
      "Epoch 101/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 488.3444 - mae: 15.7261 - val_loss: 470.9427 - val_mae: 15.6863\n",
      "Epoch 102/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 593.2857 - mae: 16.4742 - val_loss: 653.5225 - val_mae: 17.4099\n",
      "Epoch 103/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 463.1898 - mae: 15.5213 - val_loss: 435.0746 - val_mae: 15.3280\n",
      "Epoch 104/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 447.9602 - mae: 15.3316 - val_loss: 469.2053 - val_mae: 15.8290\n",
      "Epoch 105/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 445.9542 - mae: 15.2567 - val_loss: 461.4214 - val_mae: 15.5753\n",
      "Epoch 106/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 438.0663 - mae: 15.2636 - val_loss: 447.7850 - val_mae: 15.4221\n",
      "Epoch 107/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 420.5780 - mae: 15.0626 - val_loss: 426.3182 - val_mae: 15.2625\n",
      "Epoch 108/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 421.8269 - mae: 15.0753 - val_loss: 421.7333 - val_mae: 15.1557\n",
      "Epoch 109/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 433.1664 - mae: 15.2091 - val_loss: 408.6357 - val_mae: 15.0119\n",
      "Epoch 110/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 411.2971 - mae: 14.9697 - val_loss: 417.2043 - val_mae: 15.1041\n",
      "Epoch 111/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 407.7347 - mae: 14.8923 - val_loss: 432.7611 - val_mae: 15.4267\n",
      "Epoch 112/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 414.1198 - mae: 14.9335 - val_loss: 425.0663 - val_mae: 15.0182\n",
      "Epoch 113/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 411.9224 - mae: 14.9084 - val_loss: 397.7188 - val_mae: 14.7412\n",
      "Epoch 114/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 400.5457 - mae: 14.7524 - val_loss: 384.8092 - val_mae: 14.5007\n",
      "Epoch 115/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 410.6131 - mae: 14.9672 - val_loss: 409.2818 - val_mae: 15.0103\n",
      "Epoch 116/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 410.4923 - mae: 14.9659 - val_loss: 445.2915 - val_mae: 15.6901\n",
      "Epoch 117/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 404.3485 - mae: 15.0100 - val_loss: 409.5617 - val_mae: 15.0425\n",
      "Epoch 118/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 414.9106 - mae: 15.2103 - val_loss: 475.9181 - val_mae: 16.1112\n",
      "Epoch 119/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 402.7524 - mae: 14.9504 - val_loss: 375.7178 - val_mae: 14.5155\n",
      "Epoch 120/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 395.3721 - mae: 14.7762 - val_loss: 383.3652 - val_mae: 14.6762\n",
      "Epoch 121/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 390.3997 - mae: 14.7225 - val_loss: 392.4636 - val_mae: 14.6895\n",
      "Epoch 122/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 387.4263 - mae: 14.6386 - val_loss: 404.5763 - val_mae: 14.9106\n",
      "Epoch 123/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 416.9283 - mae: 15.0605 - val_loss: 417.5376 - val_mae: 15.2272\n",
      "Epoch 124/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 405.9526 - mae: 15.0046 - val_loss: 435.1086 - val_mae: 16.0361\n",
      "Epoch 125/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 398.8870 - mae: 14.8693 - val_loss: 421.1427 - val_mae: 15.2914\n",
      "Epoch 126/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 410.2534 - mae: 15.0429 - val_loss: 386.1986 - val_mae: 14.7124\n",
      "Epoch 127/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 389.3333 - mae: 14.8431 - val_loss: 413.5233 - val_mae: 15.2115\n",
      "Epoch 128/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 412.5443 - mae: 14.8051 - val_loss: 435.6136 - val_mae: 15.0273\n",
      "Epoch 129/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 432.7392 - mae: 15.0859 - val_loss: 438.9096 - val_mae: 15.2670\n",
      "Epoch 130/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 407.1198 - mae: 14.8079 - val_loss: 455.2422 - val_mae: 15.8641\n",
      "Epoch 131/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 406.8802 - mae: 14.9568 - val_loss: 409.1484 - val_mae: 15.0687\n",
      "Epoch 132/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 405.4060 - mae: 14.8846 - val_loss: 407.6095 - val_mae: 15.0770\n",
      "Epoch 133/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 381.4612 - mae: 14.5809 - val_loss: 377.8831 - val_mae: 14.5221\n",
      "Epoch 134/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 379.3661 - mae: 14.4758 - val_loss: 378.5327 - val_mae: 14.5629\n",
      "Epoch 135/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 382.3896 - mae: 14.6168 - val_loss: 395.0942 - val_mae: 15.0531\n",
      "Epoch 136/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 1216.6938 - mae: 16.6225 - val_loss: 417.0952 - val_mae: 15.2514\n",
      "Epoch 137/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 392.6693 - mae: 14.7777 - val_loss: 401.0950 - val_mae: 15.1542\n",
      "Epoch 138/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 398.7460 - mae: 14.8702 - val_loss: 440.9134 - val_mae: 15.4849\n",
      "Epoch 139/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 397.8935 - mae: 14.8963 - val_loss: 419.9134 - val_mae: 15.2566\n",
      "Epoch 140/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 379.9508 - mae: 14.6093 - val_loss: 377.8311 - val_mae: 14.6411\n",
      "Epoch 141/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 383.8828 - mae: 14.6967 - val_loss: 379.6225 - val_mae: 14.6201\n",
      "Epoch 142/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 380.6192 - mae: 14.6948 - val_loss: 395.9904 - val_mae: 15.2224\n",
      "Epoch 143/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 376.9333 - mae: 14.5542 - val_loss: 427.1871 - val_mae: 15.3227\n",
      "Epoch 144/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 374.9451 - mae: 14.5508 - val_loss: 432.8883 - val_mae: 15.7172\n",
      "Epoch 145/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 401.9022 - mae: 14.9287 - val_loss: 384.3188 - val_mae: 14.7024\n",
      "Epoch 146/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 380.4075 - mae: 14.5806 - val_loss: 371.1136 - val_mae: 14.3288\n",
      "Epoch 147/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 389.0749 - mae: 14.7299 - val_loss: 375.4007 - val_mae: 14.7550\n",
      "Epoch 148/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 367.2321 - mae: 14.4977 - val_loss: 394.1222 - val_mae: 14.7146\n",
      "Epoch 149/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 367.1056 - mae: 14.4166 - val_loss: 368.6087 - val_mae: 14.4169\n",
      "Epoch 150/150\n",
      "257/257 [==============================] - 1s 4ms/step - loss: 367.3784 - mae: 14.4323 - val_loss: 372.7383 - val_mae: 14.5489\n",
      "81/81 [==============================] - 0s 1ms/step - loss: 402.9443 - mae: 15.2019\n",
      "Mean Absolute Error on test data: 15.201879501342773\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model1 = Sequential([\n",
    "    Dense(512, activation='tanh', input_shape=(X_train.shape[1],)),\n",
    "    Dense(512, activation='tanh'),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(128, activation='tanh'),\n",
    "    Dense(64, activation='tanh'),\n",
    "    Dense(32, activation='tanh'),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "\n",
    "model1.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history1 = model1.fit(X1_train, y1_train, epochs=150, validation_split=0.2)\n",
    "\n",
    "loss1, mae1 = model1.evaluate(X1_test, y1_test)\n",
    "print(\"Mean Absolute Error on test data:\", mae1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "878033e7-2cda-4833-b516-de0604f8f30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81/81 [==============================] - 0s 2ms/step\n",
      "Relative Error on test data: 0.12321497551325813\n"
     ]
    }
   ],
   "source": [
    "# Получение прогнозов модели на тестовых данных\n",
    "y_pred1 = model1.predict(X1_test).flatten() \n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error1 = mae1 / np.abs(np.mean(y1_test))\n",
    "print(\"Relative Error on test data:\", relative_error1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8872650-cd0c-4cdc-829c-0a3724f7d83b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 19822.3555 - mae: 89.8614 - val_loss: 9087.3896 - val_mae: 51.8883\n",
      "Epoch 2/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 6481.2202 - mae: 51.3850 - val_loss: 2807.3643 - val_mae: 30.2326\n",
      "Epoch 3/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 4531.9175 - mae: 46.0402 - val_loss: 1710.3677 - val_mae: 23.7171\n",
      "Epoch 4/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 3662.5806 - mae: 40.9554 - val_loss: 1350.5211 - val_mae: 23.3604\n",
      "Epoch 5/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 3618.4011 - mae: 40.7366 - val_loss: 1119.0070 - val_mae: 20.9407\n",
      "Epoch 6/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 3383.4805 - mae: 38.9653 - val_loss: 1110.2501 - val_mae: 21.3211\n",
      "Epoch 7/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 3150.4136 - mae: 37.5324 - val_loss: 990.8906 - val_mae: 20.3274\n",
      "Epoch 8/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3243.3550 - mae: 37.5085 - val_loss: 930.6918 - val_mae: 19.8265\n",
      "Epoch 9/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 3058.4553 - mae: 36.7021 - val_loss: 817.9827 - val_mae: 17.9812\n",
      "Epoch 10/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3024.5586 - mae: 36.3439 - val_loss: 851.8237 - val_mae: 19.0430\n",
      "Epoch 11/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2956.8706 - mae: 35.8715 - val_loss: 919.2526 - val_mae: 19.7205\n",
      "Epoch 12/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2864.6465 - mae: 35.2377 - val_loss: 849.1771 - val_mae: 18.3194\n",
      "Epoch 13/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 3095.7761 - mae: 36.0693 - val_loss: 734.7405 - val_mae: 17.7168\n",
      "Epoch 14/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2906.9282 - mae: 35.5273 - val_loss: 694.5038 - val_mae: 17.5691\n",
      "Epoch 15/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2984.0508 - mae: 35.9026 - val_loss: 797.0208 - val_mae: 18.6532\n",
      "Epoch 16/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2762.5818 - mae: 34.9694 - val_loss: 844.9291 - val_mae: 19.3123\n",
      "Epoch 17/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2857.8154 - mae: 35.2034 - val_loss: 807.1589 - val_mae: 18.4103\n",
      "Epoch 18/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2853.9602 - mae: 34.9296 - val_loss: 727.6738 - val_mae: 17.9329\n",
      "Epoch 19/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2882.9204 - mae: 35.2058 - val_loss: 815.6671 - val_mae: 19.0971\n",
      "Epoch 20/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2753.1750 - mae: 35.0195 - val_loss: 695.9913 - val_mae: 17.1484\n",
      "Epoch 21/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2761.6255 - mae: 34.5624 - val_loss: 783.3207 - val_mae: 18.2610\n",
      "Epoch 22/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2733.8003 - mae: 34.5756 - val_loss: 688.1374 - val_mae: 16.9922\n",
      "Epoch 23/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2667.6052 - mae: 33.7002 - val_loss: 657.6107 - val_mae: 16.7256\n",
      "Epoch 24/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2771.1589 - mae: 34.3295 - val_loss: 813.9826 - val_mae: 18.5987\n",
      "Epoch 25/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2696.2969 - mae: 34.1313 - val_loss: 808.9351 - val_mae: 18.2147\n",
      "Epoch 26/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2726.4438 - mae: 34.1048 - val_loss: 963.3181 - val_mae: 20.7542\n",
      "Epoch 27/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2707.1167 - mae: 34.1924 - val_loss: 786.8317 - val_mae: 18.6768\n",
      "Epoch 28/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2669.7205 - mae: 33.8137 - val_loss: 648.9487 - val_mae: 16.6579\n",
      "Epoch 29/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2789.7561 - mae: 34.0339 - val_loss: 757.4130 - val_mae: 17.4239\n",
      "Epoch 30/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2719.8904 - mae: 33.5978 - val_loss: 765.1319 - val_mae: 17.8557\n",
      "Epoch 31/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2760.9312 - mae: 34.1257 - val_loss: 683.4800 - val_mae: 16.6281\n",
      "Epoch 32/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2652.9048 - mae: 33.5415 - val_loss: 641.4091 - val_mae: 16.3617\n",
      "Epoch 33/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2545.7498 - mae: 33.0520 - val_loss: 653.9670 - val_mae: 16.5556\n",
      "Epoch 34/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2764.7886 - mae: 33.7070 - val_loss: 713.1438 - val_mae: 17.3577\n",
      "Epoch 35/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2567.5359 - mae: 33.0615 - val_loss: 752.9446 - val_mae: 17.5831\n",
      "Epoch 36/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2715.0625 - mae: 33.6030 - val_loss: 829.9487 - val_mae: 18.8397\n",
      "Epoch 37/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2723.3357 - mae: 33.6696 - val_loss: 717.7576 - val_mae: 16.8600\n",
      "Epoch 38/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2575.0891 - mae: 32.9874 - val_loss: 726.3792 - val_mae: 17.2467\n",
      "Epoch 39/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2631.0378 - mae: 33.0970 - val_loss: 784.4831 - val_mae: 18.3137\n",
      "Epoch 40/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2604.9988 - mae: 32.9074 - val_loss: 767.2905 - val_mae: 17.9990\n",
      "Epoch 41/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2808.5713 - mae: 33.8427 - val_loss: 742.8813 - val_mae: 17.5483\n",
      "Epoch 42/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2701.0400 - mae: 33.5195 - val_loss: 771.7593 - val_mae: 17.8841\n",
      "Epoch 43/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2652.5754 - mae: 33.1591 - val_loss: 718.6484 - val_mae: 17.4076\n",
      "Epoch 44/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2599.8508 - mae: 32.8230 - val_loss: 689.3265 - val_mae: 17.1093\n",
      "Epoch 45/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2594.5825 - mae: 33.2257 - val_loss: 628.4164 - val_mae: 16.1687\n",
      "Epoch 46/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2590.9282 - mae: 32.7482 - val_loss: 679.2857 - val_mae: 16.6135\n",
      "Epoch 47/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2550.7041 - mae: 32.4914 - val_loss: 734.4777 - val_mae: 17.4370\n",
      "Epoch 48/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2584.9072 - mae: 32.4191 - val_loss: 707.5408 - val_mae: 17.2373\n",
      "Epoch 49/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2527.3101 - mae: 32.6149 - val_loss: 683.4207 - val_mae: 16.4536\n",
      "Epoch 50/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2452.5413 - mae: 32.3652 - val_loss: 626.2024 - val_mae: 16.3871\n",
      "Epoch 51/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2530.4888 - mae: 32.3597 - val_loss: 630.6906 - val_mae: 16.1031\n",
      "Epoch 52/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2618.5518 - mae: 32.6847 - val_loss: 747.7027 - val_mae: 17.4341\n",
      "Epoch 53/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2532.8164 - mae: 32.3139 - val_loss: 664.1387 - val_mae: 16.4223\n",
      "Epoch 54/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2498.3098 - mae: 32.1614 - val_loss: 846.1602 - val_mae: 18.9956\n",
      "Epoch 55/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2565.0603 - mae: 32.0773 - val_loss: 772.0321 - val_mae: 17.4441\n",
      "Epoch 56/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2453.1448 - mae: 32.0968 - val_loss: 708.8669 - val_mae: 16.9224\n",
      "Epoch 57/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2492.8311 - mae: 32.1235 - val_loss: 719.3154 - val_mae: 17.1793\n",
      "Epoch 58/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2507.0625 - mae: 32.0815 - val_loss: 681.5542 - val_mae: 16.3995\n",
      "Epoch 59/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2472.0476 - mae: 31.5602 - val_loss: 679.9173 - val_mae: 17.0039\n",
      "Epoch 60/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2446.1470 - mae: 31.7692 - val_loss: 660.2999 - val_mae: 16.9262\n",
      "Epoch 61/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2376.9531 - mae: 31.4176 - val_loss: 746.4313 - val_mae: 17.5295\n",
      "Epoch 62/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2461.9375 - mae: 31.7262 - val_loss: 683.6310 - val_mae: 16.5572\n",
      "Epoch 63/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2366.5095 - mae: 31.1208 - val_loss: 691.2115 - val_mae: 16.7065\n",
      "Epoch 64/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2306.5281 - mae: 31.3353 - val_loss: 637.5734 - val_mae: 16.2814\n",
      "Epoch 65/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2538.9990 - mae: 32.3858 - val_loss: 736.9775 - val_mae: 17.4862\n",
      "Epoch 66/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2418.9524 - mae: 31.6131 - val_loss: 780.3582 - val_mae: 18.2018\n",
      "Epoch 67/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2340.6936 - mae: 31.1924 - val_loss: 649.5095 - val_mae: 16.3131\n",
      "Epoch 68/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2371.3689 - mae: 31.0653 - val_loss: 682.4470 - val_mae: 16.7156\n",
      "Epoch 69/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2329.8103 - mae: 30.9447 - val_loss: 718.9456 - val_mae: 17.3662\n",
      "Epoch 70/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2292.2407 - mae: 30.8980 - val_loss: 691.4730 - val_mae: 16.5402\n",
      "Epoch 71/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2423.8975 - mae: 31.2704 - val_loss: 720.2695 - val_mae: 16.7928\n",
      "Epoch 72/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2364.2131 - mae: 30.8456 - val_loss: 722.2675 - val_mae: 16.9996\n",
      "Epoch 73/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2346.8813 - mae: 31.3247 - val_loss: 707.9108 - val_mae: 16.3685\n",
      "Epoch 74/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2360.2783 - mae: 30.9291 - val_loss: 767.2145 - val_mae: 17.5102\n",
      "Epoch 75/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2251.4758 - mae: 30.5496 - val_loss: 770.7529 - val_mae: 17.6678\n",
      "Epoch 76/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2364.0083 - mae: 31.5297 - val_loss: 708.8373 - val_mae: 17.4643\n",
      "Epoch 77/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2306.5183 - mae: 30.8982 - val_loss: 804.9783 - val_mae: 18.2133\n",
      "Epoch 78/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2287.9185 - mae: 30.8254 - val_loss: 675.5653 - val_mae: 16.7100\n",
      "Epoch 79/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2229.6123 - mae: 30.7421 - val_loss: 795.5073 - val_mae: 18.1799\n",
      "Epoch 80/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2382.1790 - mae: 31.1262 - val_loss: 715.2111 - val_mae: 17.3253\n",
      "Epoch 81/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2276.6816 - mae: 30.4904 - val_loss: 667.1109 - val_mae: 16.4235\n",
      "Epoch 82/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2286.9155 - mae: 30.4165 - val_loss: 756.8176 - val_mae: 17.4850\n",
      "Epoch 83/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2292.5215 - mae: 30.5833 - val_loss: 793.6903 - val_mae: 17.7670\n",
      "Epoch 84/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2328.4409 - mae: 30.6693 - val_loss: 718.6773 - val_mae: 16.6085\n",
      "Epoch 85/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2262.8003 - mae: 30.2511 - val_loss: 706.9473 - val_mae: 16.6193\n",
      "Epoch 86/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2224.4407 - mae: 30.2928 - val_loss: 844.0477 - val_mae: 18.1145\n",
      "Epoch 87/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2302.9355 - mae: 30.5277 - val_loss: 686.6039 - val_mae: 16.5854\n",
      "Epoch 88/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2125.3450 - mae: 30.0754 - val_loss: 683.3352 - val_mae: 16.2279\n",
      "Epoch 89/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2363.4448 - mae: 30.7923 - val_loss: 752.0287 - val_mae: 17.3356\n",
      "Epoch 90/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2183.6348 - mae: 29.9397 - val_loss: 626.4419 - val_mae: 16.2169\n",
      "Epoch 91/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2351.9424 - mae: 30.9356 - val_loss: 716.0779 - val_mae: 16.6067\n",
      "Epoch 92/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2226.3225 - mae: 30.2969 - val_loss: 704.6324 - val_mae: 16.6599\n",
      "Epoch 93/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2218.1384 - mae: 30.1079 - val_loss: 690.8727 - val_mae: 16.4507\n",
      "Epoch 94/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2151.7346 - mae: 29.7905 - val_loss: 782.8801 - val_mae: 17.7975\n",
      "Epoch 95/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2104.2854 - mae: 29.5237 - val_loss: 720.6760 - val_mae: 16.9517\n",
      "Epoch 96/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2116.7429 - mae: 29.6572 - val_loss: 683.4315 - val_mae: 16.3205\n",
      "Epoch 97/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2253.4958 - mae: 29.8763 - val_loss: 675.9005 - val_mae: 16.1757\n",
      "Epoch 98/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2215.5105 - mae: 29.9193 - val_loss: 727.9349 - val_mae: 17.3540\n",
      "Epoch 99/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2151.2610 - mae: 29.7531 - val_loss: 676.4408 - val_mae: 16.6274\n",
      "Epoch 100/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2098.5173 - mae: 29.6380 - val_loss: 694.2358 - val_mae: 16.6384\n",
      "Epoch 101/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2159.1208 - mae: 29.5892 - val_loss: 710.8575 - val_mae: 16.6658\n",
      "Epoch 102/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2168.2368 - mae: 30.0881 - val_loss: 675.6886 - val_mae: 16.2088\n",
      "Epoch 103/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2149.0266 - mae: 29.9286 - val_loss: 743.0419 - val_mae: 17.3091\n",
      "Epoch 104/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2207.7661 - mae: 30.0103 - val_loss: 772.9379 - val_mae: 17.3534\n",
      "Epoch 105/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2174.7717 - mae: 29.7272 - val_loss: 659.1479 - val_mae: 16.1252\n",
      "Epoch 106/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2156.9026 - mae: 29.8896 - val_loss: 777.7957 - val_mae: 18.1115\n",
      "Epoch 107/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2158.7708 - mae: 29.6587 - val_loss: 752.7150 - val_mae: 17.6043\n",
      "Epoch 108/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2138.1248 - mae: 29.7640 - val_loss: 708.1205 - val_mae: 16.7549\n",
      "Epoch 109/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2129.6553 - mae: 29.5331 - val_loss: 623.8049 - val_mae: 15.6724\n",
      "Epoch 110/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2125.2595 - mae: 29.4934 - val_loss: 651.9866 - val_mae: 16.0925\n",
      "Epoch 111/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2168.4099 - mae: 29.4610 - val_loss: 685.4107 - val_mae: 16.8087\n",
      "Epoch 112/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2137.1868 - mae: 29.3820 - val_loss: 646.7999 - val_mae: 16.2213\n",
      "Epoch 113/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2227.0459 - mae: 29.7285 - val_loss: 699.4777 - val_mae: 16.6392\n",
      "Epoch 114/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2135.3333 - mae: 29.4714 - val_loss: 751.2296 - val_mae: 17.7069\n",
      "Epoch 115/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2098.4375 - mae: 29.4740 - val_loss: 656.5852 - val_mae: 16.2183\n",
      "Epoch 116/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2159.3347 - mae: 29.6589 - val_loss: 631.9257 - val_mae: 16.0079\n",
      "Epoch 117/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2088.1492 - mae: 29.0757 - val_loss: 642.3371 - val_mae: 15.9246\n",
      "Epoch 118/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2040.0856 - mae: 29.0028 - val_loss: 721.8320 - val_mae: 16.8490\n",
      "Epoch 119/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2101.9910 - mae: 29.3231 - val_loss: 751.5895 - val_mae: 17.3985\n",
      "Epoch 120/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2042.6920 - mae: 28.9631 - val_loss: 764.0502 - val_mae: 17.4578\n",
      "Epoch 121/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2093.7644 - mae: 29.0022 - val_loss: 686.2993 - val_mae: 16.4611\n",
      "Epoch 122/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2159.1558 - mae: 29.8029 - val_loss: 803.0106 - val_mae: 18.0992\n",
      "Epoch 123/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2121.6348 - mae: 29.3605 - val_loss: 821.7470 - val_mae: 17.8251\n",
      "Epoch 124/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2187.5522 - mae: 29.3729 - val_loss: 722.4736 - val_mae: 16.4659\n",
      "Epoch 125/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2107.1191 - mae: 29.1817 - val_loss: 686.3000 - val_mae: 16.1383\n",
      "Epoch 126/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2051.3594 - mae: 28.7354 - val_loss: 711.0184 - val_mae: 16.7081\n",
      "Epoch 127/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2076.5215 - mae: 28.6719 - val_loss: 687.3770 - val_mae: 16.3948\n",
      "Epoch 128/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2121.3293 - mae: 29.1596 - val_loss: 695.0654 - val_mae: 16.2071\n",
      "Epoch 129/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2180.3059 - mae: 29.7195 - val_loss: 847.4712 - val_mae: 18.1863\n",
      "Epoch 130/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 1990.5475 - mae: 28.4467 - val_loss: 640.0643 - val_mae: 15.6336\n",
      "Epoch 131/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2064.1292 - mae: 28.7225 - val_loss: 655.6226 - val_mae: 16.0164\n",
      "Epoch 132/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2032.1676 - mae: 28.7034 - val_loss: 659.7404 - val_mae: 15.8660\n",
      "Epoch 133/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2134.3076 - mae: 29.2345 - val_loss: 669.9752 - val_mae: 16.0568\n",
      "Epoch 134/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2020.7458 - mae: 28.8364 - val_loss: 673.3718 - val_mae: 16.0820\n",
      "Epoch 135/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2076.5620 - mae: 28.9078 - val_loss: 694.0095 - val_mae: 16.3298\n",
      "Epoch 136/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2074.3445 - mae: 28.9365 - val_loss: 690.2266 - val_mae: 16.4743\n",
      "Epoch 137/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2014.3485 - mae: 28.5500 - val_loss: 638.1356 - val_mae: 15.7069\n",
      "Epoch 138/150\n",
      "257/257 [==============================] - 2s 6ms/step - loss: 2094.8169 - mae: 29.1749 - val_loss: 672.2054 - val_mae: 15.9778\n",
      "Epoch 139/150\n",
      "257/257 [==============================] - 1s 5ms/step - loss: 2154.4668 - mae: 29.3598 - val_loss: 695.8654 - val_mae: 16.2590\n",
      "Epoch 140/150\n",
      "257/257 [==============================] - 1s 6ms/step - loss: 2079.6548 - mae: 28.8411 - val_loss: 715.9311 - val_mae: 16.4898\n",
      "Epoch 141/150\n",
      "196/257 [=====================>........] - ETA: 0s - loss: 2142.5249 - mae: 28.8963"
     ]
    }
   ],
   "source": [
    "#С нормализованными данными\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model1_0 = Sequential([\n",
    "    Dense(512, activation='tanh', kernel_regularizer=regularizers.l2(0.01), input_shape=(X_0_train.shape[1],)),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(128, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(64, activation='tanh', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dropout(0.5),\n",
    "    Dense(1)  \n",
    "])\n",
    "\n",
    "\n",
    "model1_0.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "history1_0 = model1_0.fit(X1_0_train, y1_0_train, epochs=150, validation_split=0.2)\n",
    "\n",
    "loss1_0, mae1_0 = model1_0.evaluate(X1_0_test, y1_0_test)\n",
    "print(\"Mean Absolute Error on test data:\", mae1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae445e44-c487-484c-aa98-2f47a295bc21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение прогнозов модели на тестовых данных\n",
    "y_pred1_0 = model.predict(X1_0_test).flatten() \n",
    "\n",
    "# Рассчет относительной ошибки\n",
    "relative_error1_0 = mae1_0 / np.abs(np.mean(y1_0_test))\n",
    "print(\"Relative Error on test data:\", relative_error1_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ac02dc-4871-45ce-a818-6952161e4abc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ace29f-05ec-4705-8480-72598860eb70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d7d1fb-10d1-45a6-b147-355ffb9bf4a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
